{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AeneasWeiChiHsu/AeneasWeiChiHsu/blob/main/%5BDouble_CNN%5DEvolution_Module_%5BMain_Structure%5D_Baseline_Dropout_Triple_Ensemble_Randomized_Contrast_Enhance_Double_Residual_Prioritized_Multi_Step_Double_Dual_Head_DQN_CarRacing_RL_Project_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V84pDLi2F3WZ"
      },
      "source": [
        "âš ï¸ **ä¸è¦ç†¬å¤œå¯«codeã€‚**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWczEXUs-bYo"
      },
      "source": [
        "# Introduction: Double Duel DQN on Car Racing v3\n",
        "\n",
        "Goal: Average Reward 600+ (more is good) in randomized = True (real test)\n",
        "\n",
        "Environment: CarRacing-v3 with domain_randomize=True\n",
        "\n",
        "I adopt a hybrid approach, for more detail, check this one: https://arxiv.org/pdf/1710.02298ã€‚\n",
        "\n",
        "The logic is following: it must work first then we go further.\n",
        "\n",
        "å…ˆæ±‚ä¸å‚·èº«é«”å†æ±‚æœ‰ç™‚æ•ˆã€‚\n",
        "\n",
        "â¬†ï¸ DQN (Basic)\n",
        "\n",
        "â¬†ï¸ Dual Head (Q-head: Advanced, Value head -- prevent from overestimate Q-value)\n",
        "\n",
        "â¬†ï¸ Double Network (Model Net + Target Net)\n",
        "\n",
        "â¬†ï¸ Multiple Step Memory (SARSA)\n",
        "\n",
        "â¬†ï¸ Prioritization (Learning from useful experience)\n",
        "\n",
        "â¬†ï¸ Epsilon modification (Adaptive exploration)\n",
        "\n",
        "â¬†ï¸ Reward shaping (Incentive)\n",
        "\n",
        "â¬†ï¸ Residual Connection (Feature engineering)\n",
        "\n",
        "â¬†ï¸ Contrast Enhancement (Inspired by eye)\n",
        "\n",
        "â¬†ï¸ Ensemble (Triple Advantage/Value Network)\n",
        "\n",
        "â¬†ï¸ Training on Randomized Environment (For Generalization)\n",
        "\n",
        "ðŸ§ª Vision Enhancement  (Visual Cortex)\n",
        "\n",
        "ðŸ§ª Evolution\n",
        "\n",
        "---\n",
        "I tested the following listed elements, but it does not works well. Perhaps it is due to un-fine-tuned parameters. However, at this stage, for some reasons, I decided not to use them. In future, it is possible to add them back.\n",
        "\n",
        "â¬‡ï¸ Q-Distribution (not used) -- I want to remain the duelling structure first.\n",
        "\n",
        "â¬‡ï¸ Noisy Net (Unstable)\n",
        "\n",
        "â¬‡ï¸ Attention Mechanism (backfire)\n",
        "\n",
        "> I decide to train an agent to drive because I don't know how to drive.\n",
        "\n",
        "IBM on Coursera  [Deep Learning with Keras and TensorFlow](https://www.coursera.org/learn/building-deep-learning-models-with-tensorflow/home)\n",
        "\n",
        "Section [Building a Deep Q-Network with Keras](https://www.coursera.org/learn/building-deep-learning-models-with-tensorflow/ungradedLti/AlszB/lab-building-a-deep-q-network-with-keras)\n",
        "\n",
        "University of Alberta on Coursera [Fundamentals of Reinforcement Learning](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPZEpyZnActC"
      },
      "source": [
        "# Goal: Deep Q-Network with Keras\n",
        "\n",
        "The story started when I took a course called Control Theory â€” though in practice, it was all about reinforcement learning. At the time, I didnâ€™t understand much. I just wanted to finish the final project: solving the CarRacing environment (v2).\n",
        "\n",
        "While looking for help, I found a public DualDQN implementation on GitHub. It looked cool, so I tweaked it a little and used it for my project. Technically, it wasnâ€™t my agent that solved the problem â€” it was someone elseâ€™s PyTorch code that did most of the heavy lifting.\n",
        "\n",
        "But for some reason, Iâ€™ve never liked PyTorch. So now, Iâ€™m rebuilding the entire thing from scratch â€” in TensorFlow and Keras, the way I think it should be.\n",
        "\n",
        "æ•…äº‹æ˜¯é€™æ¨£çš„ï¼Œæˆ‘ä¹‹å‰ä¸ŠéŽä¸€é–€èª²ï¼Œå«åšã€ŠæŽ§åˆ¶ç†è«–ã€‹ï¼Œä½†æ˜¯å…§å®¹éƒ½æ˜¯åœ¨è¬› Reinforcement Learningã€‚\n",
        "ä¸éŽé‚£å€‹æ™‚å€™æˆ‘ä»€éº¼éƒ½ä¸æ‡‚ï¼Œåªæ˜¯æƒ³è¦åšå®ŒæœŸæœ«å°ˆé¡Œï¼šCarRacingã€‚\n",
        "æ‰€ä»¥æˆ‘åœ¨Githubä¸Šé¢çœ‹åˆ°æœ‰äºº share äº†ä¸€æ®µ DualDQN çš„codeï¼Œæˆ‘è¦ºå¾—å¾ˆæ£’ï¼Œæˆ‘å°±æ‹¿ä¾†æ”¹ã€‚\n",
        "æœ€å¾Œé‚£å€‹ä¸æ˜¯æˆ‘çš„ Agent å¹«æˆ‘å®Œæˆäº†æˆ‘çš„ Final Projectã€‚\n",
        "ä¸éŽé‚£æ®µ Code æ˜¯ç”¨ pytorch å¯«çš„ã€‚ä½†æ˜¯åŸºæ–¼æŸäº›ç†ç”±æˆ‘ä¸å–œæ­¡ pytorchã€‚\n",
        "ç¾åœ¨æˆ‘è¦åšé€™å€‹ Projectã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gU12oLKksZt"
      },
      "source": [
        "# Box2D\n",
        "\n",
        "https://gymnasium.farama.org/environments/box2d/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv-J6unL-UAN"
      },
      "outputs": [],
      "source": [
        "# Instal Gym\n",
        "!pip install gymnasium\n",
        "! pip install swig\n",
        "! pip install gymnasium[box2d]\n",
        "# Install OpenCV\n",
        "!pip install opencv-python\n",
        "\n",
        "# ==== Note ====\n",
        "# If you are run it in local jupyter lab, uncomment this line\n",
        "#!pip install box2d pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrGVD9fWBy98"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# ==== Layers ====\n",
        "from tensorflow.keras.layers import Input, InputLayer, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose, LeakyReLU, Dropout, BatchNormalization, ReLU, MaxPooling2D, UpSampling2D, Concatenate, Activation, Lambda, Add, LayerNormalization, LeakyReLU, AveragePooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D, ZeroPadding2D, Cropping2D, ZeroPadding1D, Cropping1D, Layer, Average, AveragePooling1D\n",
        "\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "\n",
        "# ==== Optimizer ====\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
        "\n",
        "# ==== Callbacks ====\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ==== Saving ====\n",
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "# ==== Get File ====\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# ==== tqdm ====\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "# ==== Counter ====\n",
        "from collections import Counter\n",
        "\n",
        "# ==== Statistics ====\n",
        "import scipy.stats\n",
        "\n",
        "# ==== Video ====\n",
        "import imageio\n",
        "import os\n",
        "import cv2 # Import cv2 here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey0bYCJ_j5tr"
      },
      "source": [
        "# Environment\n",
        "\n",
        "https://gymnasium.farama.org/environments/box2d/car_racing/\n",
        "\n",
        "Actions = [do nothing, left, right, gas, brake]\n",
        "\n",
        "Observation = **A top-down 96x96 RGB image of the car and race track.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS2mmrWhjBf7"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v3\",\n",
        "               render_mode= None,\n",
        "               lap_complete_percent=0.95,\n",
        "               domain_randomize=True,\n",
        "               continuous=False)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qnGsGcdpMLE"
      },
      "source": [
        "# Model\n",
        "\n",
        "\n",
        "â¬‡ï¸ The basic architecute is Duel DQN. Therefore, for each Q-head, we will have two subnet: advanced and value.\n",
        "\n",
        "Furthermore, I use Stack Frame (RGB). Then the CNN input is 96 x 96 x 12 (for 4 frames)\n",
        "\n",
        "I use Residual Connect and Constrast Enhancement. I suppose it helps. This idea is inspired by bio-visual system. How you see this world will determine your brain structure.\n",
        "\n",
        "---\n",
        "â¬‡ï¸ My feeling\n",
        "\n",
        "ðŸ“¦ It is like a box. You never know what you will find inside the box.\n",
        "\n",
        "ðŸ“ˆ OR it is just like a stack."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contrast Enhancement"
      ],
      "metadata": {
        "id": "2-lOzwVyGSBe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ7N2VQRTjFT"
      },
      "outputs": [],
      "source": [
        "# ==== Nonlinear Contrast Enhancement ====\n",
        "\n",
        "# ==== Register your layer ====\n",
        "@register_keras_serializable()\n",
        "\n",
        "class ContrastEnhance(Layer):\n",
        "    def __init__(self, alpha=1.5, **kwargs):\n",
        "        super(ContrastEnhance, self).__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def call(self, x):\n",
        "        mean = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
        "        return tf.clip_by_value(self.alpha * (x - mean) + mean, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Visualization of Contrast Enhancement ====\n",
        "# (batch_size, height, width, channels)\n",
        "img = tf.random.uniform((1, 96, 96, 3), minval=0.3, maxval=0.7)  # soft grayish noise\n",
        "\n",
        "# Apply Contrast Enhance\n",
        "layer = ContrastEnhance(alpha=2.0)\n",
        "enhanced_img = layer(img)\n",
        "\n",
        "# ==== Plot Original vs Enhanced ====\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original\")\n",
        "plt.imshow(img[0].numpy())\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Enhanced (alpha=2.0)\")\n",
        "plt.imshow(enhanced_img[0].numpy())\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qZ-uWS63Fs2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dueling Q output"
      ],
      "metadata": {
        "id": "_tCVlkMFHAY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Dueling Q Output ====\n",
        "\n",
        "# ==== Register your layer ====\n",
        "@register_keras_serializable()\n",
        "\n",
        "def dueling_q_output(x):\n",
        "    v, a = x\n",
        "    return v + (a - tf.reduce_mean(a, axis=1, keepdims=True))"
      ],
      "metadata": {
        "id": "Q3T0VojPGxLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sobel Layer"
      ],
      "metadata": {
        "id": "ogCfUgykRp4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Sober ====\n",
        "\n",
        "# ==== Register your layer ====\n",
        "@register_keras_serializable()\n",
        "\n",
        "class SobelEdgeLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Sobel kernels (3x3)\n",
        "        self.sobel_x = tf.constant([[[-1, 0, 1],\n",
        "                                     [-2, 0, 2],\n",
        "                                     [-1, 0, 1]]], dtype=tf.float32)\n",
        "        self.sobel_y = tf.constant([[[-1, -2, -1],\n",
        "                                     [ 0,  0,  0],\n",
        "                                     [ 1,  2,  1]]], dtype=tf.float32)\n",
        "        self.sobel_45 = tf.constant([[[-2, -1, 0],\n",
        "                              [-1,  0, 1],\n",
        "                              [ 0,  1, 2]]], dtype=tf.float32)\n",
        "\n",
        "        self.sobel_135 = tf.constant([[[ 0, -1, -2],\n",
        "                               [ 1,  0, -1],\n",
        "                               [ 2,  1,  0]]], dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Input shape: (batch, H, W, C)\n",
        "        channels = x.shape[-1]\n",
        "        kernel_x = tf.expand_dims(self.sobel_x, axis=-1)\n",
        "        kernel_y = tf.expand_dims(self.sobel_y, axis=-1)\n",
        "\n",
        "        # Repeat for depthwise convolution\n",
        "        kernel_x = tf.repeat(kernel_x, repeats=channels, axis=-1)\n",
        "        kernel_y = tf.repeat(kernel_y, repeats=channels, axis=-1)\n",
        "\n",
        "        kernel_x = tf.reshape(kernel_x, [3, 3, channels, 1])\n",
        "        kernel_y = tf.reshape(kernel_y, [3, 3, channels, 1])\n",
        "\n",
        "        # Apply Sobel filters\n",
        "        gx = tf.nn.depthwise_conv2d(x, kernel_x, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        gy = tf.nn.depthwise_conv2d(x, kernel_y, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "\n",
        "        grad_magnitude = tf.sqrt(tf.square(gx) + tf.square(gy) + 1e-6)  # é˜²æ­¢ sqrt(0)\n",
        "\n",
        "        # Test\n",
        "        x = 0.2*(1-grad_magnitude)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "o0Yobg0JRt_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visual Enhance Block"
      ],
      "metadata": {
        "id": "j-NWsEczgXOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class VisualEnhanceBlock(Layer):\n",
        "    def __init__(self, gene_code=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # ==== Defaul Gene ====\n",
        "        if gene_code is None:\n",
        "            gene_code = [2, 0, 2.0, False]  # [contrast_times, sobel_times, alpha, use_residual]\n",
        "\n",
        "        # ==== unpack ====\n",
        "        self.contrast_times = int(gene_code[0])\n",
        "        self.sobel_times = int(gene_code[1])\n",
        "        self.alpha = float(gene_code[2])\n",
        "        self.use_residual = bool(gene_code[3])\n",
        "\n",
        "        # ==== layer list ====\n",
        "        self.contrast_layers = [ContrastEnhance(alpha=self.alpha) for _ in range(self.contrast_times)]\n",
        "        self.sobel_layers = [SobelEdgeLayer() for _ in range(self.sobel_times)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for contrast in self.contrast_layers:\n",
        "            x = contrast(x)\n",
        "\n",
        "        for sobel in self.sobel_layers:\n",
        "            x = sobel(x) + x  # Enhance edge\n",
        "\n",
        "        if self.use_residual:\n",
        "            return Add()([x, inputs])\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"gene_code\": [self.contrast_times, self.sobel_times, self.alpha, self.use_residual]\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "wM067DITgdBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import get_file\n",
        "# === Load fallback image ===\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\"\n",
        "image_path = get_file(\"apple.jpg\", origin=image_url)\n",
        "\n",
        "img_raw = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
        "img = tf.image.resize(img_raw, [96, 96]) / 255.0\n",
        "img = tf.expand_dims(img, axis=0)  # (1, 96, 96, 1)\n",
        "\n",
        "# === Apply Sobel Filter ===\n",
        "sobel_layer = SobelEdgeLayer()\n",
        "edges = 1 -  sobel_layer(img) + img\n",
        "\n",
        "# === Show Result ===\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original (Apple)\")\n",
        "plt.imshow(img[0, ..., 0], cmap = 'gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Sobel Edge\")\n",
        "plt.imshow(edges[0, ..., 0], cmap = 'gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xpegQBEbSDvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Apple Image ===\n",
        "apple_path = tf.keras.utils.get_file(\"apple.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\")\n",
        "\n",
        "# To Gray + resize\n",
        "image = tf.image.decode_jpeg(tf.io.read_file(apple_path), channels=3)\n",
        "image = tf.image.rgb_to_grayscale(image)\n",
        "image = tf.image.resize(image, [96, 96]) / 255.0\n",
        "image = tf.expand_dims(image, axis=0)  # shape = (1, 96, 96, 1)\n",
        "\n",
        "# Sent to Conv2D\n",
        "image = tf.repeat(image, repeats=8, axis=-1)  # shape = (1, 96, 96, 8)\n",
        "\n",
        "# === Test Layer  ===\n",
        "residual = image  # Preserve original input\n",
        "x = ContrastEnhance(alpha=2.0)(image)\n",
        "\n",
        "#x = ContrastEnhance(alpha=2.0)(x)\n",
        "\n",
        "x = SobelEdgeLayer()(x) + x\n",
        "\n",
        "\n",
        "x = tf.keras.layers.Add()([x, residual])  # Residual short cut\n",
        "#x = tf.clip_by_value(x, 0.0, 1.0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(3):\n",
        "    # Original\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.title(f\"Original (channel {i})\")\n",
        "    plt.imshow(image[0, :, :, i].numpy(), cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Processed\n",
        "    plt.subplot(2, 3, i + 4)\n",
        "    plt.title(f\"Contrast+Sobel (channel {i})\")\n",
        "    plt.imshow(x[0, :, :, i].numpy(), cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lLEx7mo_w76r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Apple Image ===\n",
        "apple_path = tf.keras.utils.get_file(\"apple.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\")\n",
        "\n",
        "# To Gray + resize\n",
        "image = tf.image.decode_jpeg(tf.io.read_file(apple_path), channels=3)\n",
        "image = tf.image.rgb_to_grayscale(image)\n",
        "image = tf.image.resize(image, [96, 96]) / 255.0\n",
        "image = tf.expand_dims(image, axis=0)  # shape = (1, 96, 96, 1)\n",
        "\n",
        "# Sent to Conv2D\n",
        "image = tf.repeat(image, repeats=8, axis=-1)  # shape = (1, 96, 96, 8)\n",
        "\n",
        "visual_gene = [1, 1, 2.0, True]\n",
        "visual_block = VisualEnhanceBlock(gene_code=visual_gene)\n",
        "x = visual_block(image)\n",
        "\n",
        "residual = image  # Preserve original input\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(3):\n",
        "    # Original\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.title(f\"Original (channel {i})\")\n",
        "    plt.imshow(image[0, :, :, i].numpy(), cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Processed\n",
        "    plt.subplot(2, 3, i + 4)\n",
        "    plt.title(f\"Contrast+Sobel (channel {i})\")\n",
        "    plt.imshow(x[0, :, :, i].numpy(), cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CRjrv6v6j3CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-head"
      ],
      "metadata": {
        "id": "0MiFnQ6KopfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Q head ====\n",
        "\n",
        "def build_q_head(x, action_size, head_id=0, use_dropout=False):\n",
        "    a = Dense(256, activation='relu', name=f'a_{head_id}_dense1')(x)\n",
        "    a = Dense(128, activation='relu', name=f'a_{head_id}_dense2')(a)\n",
        "    if use_dropout:\n",
        "        a = Dropout(0.2, name=f'a_{head_id}_drop')(a)\n",
        "    a = Dense(action_size, name=f'a_{head_id}_out')(a)\n",
        "\n",
        "    v = Dense(256, activation='relu', name=f'v_{head_id}_dense1')(x)\n",
        "    v = Dense(128, name=f'v_{head_id}_dense2')(v)\n",
        "    if use_dropout:\n",
        "        v = Dropout(0.2, name=f'v_{head_id}_drop')(v)\n",
        "    v = Dense(1, name=f'v_{head_id}_out')(v)\n",
        "\n",
        "    q = Lambda(dueling_q_output, name=f'q_{head_id}')([v, a])\n",
        "    return q"
      ],
      "metadata": {
        "id": "JA-6UKNSovyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Res Block"
      ],
      "metadata": {
        "id": "raCO9wGTiN7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def res_block(x, filters):\n",
        "    shortcut = x\n",
        "    x = Conv2D(filters, 3, padding='same', activation=None)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(filters, 3, padding='same', activation=None)(x)\n",
        "    x = Add()([x, shortcut])\n",
        "    x = ReLU()(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "HdXOawQBiV4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type-0 Q-Head"
      ],
      "metadata": {
        "id": "o2L-FzcAvQhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Type-0 Q-head ===\n",
        "def build_q_head_type0(x, action_size = 5, head_id=\"1\"):\n",
        "    # Advantage Stream\n",
        "    a = Dense(256, name=f\"a_{head_id}_dense1\")(x)\n",
        "    a = Dense(action_size, name=f\"a_{head_id}_out\")(a)\n",
        "\n",
        "    # Value Stream\n",
        "    v = Dense(256, activation='relu', name=f\"v_{head_id}_dense1\")(x)\n",
        "    v = Dense(1, name=f\"v_{head_id}_out\")(v)\n",
        "\n",
        "    # Combine into Q-values\n",
        "    q = Lambda(dueling_q_output, name=f\"q_{head_id}\")([v, a])\n",
        "    return q"
      ],
      "metadata": {
        "id": "3Z9cqbCvvUAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type-1 Q-Head"
      ],
      "metadata": {
        "id": "xCU4vy10thGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Type-1 Q-head ===\n",
        "def build_q_head_type1(x, action_size = 5, head_id=\"1\"):\n",
        "    # Advantage Stream\n",
        "    a = Dense(256, name=f\"a_{head_id}_dense1\")(x)\n",
        "    a = Dense(128, activation='relu', name=f\"a_{head_id}_dense2\")(a)\n",
        "    a = Dense(action_size, name=f\"a_{head_id}_out\")(a)\n",
        "\n",
        "    # Value Stream\n",
        "    v = Dense(256, activation='relu', name=f\"v_{head_id}_dense1\")(x)\n",
        "    v = Dense(128, name=f\"v_{head_id}_dense2\")(v)\n",
        "    v = Dense(1, name=f\"v_{head_id}_out\")(v)\n",
        "\n",
        "    # Combine into Q-values\n",
        "    q = Lambda(dueling_q_output, name=f\"q_{head_id}\")([v, a])\n",
        "    return q"
      ],
      "metadata": {
        "id": "02RpZXqitkX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type-2 Q-Head"
      ],
      "metadata": {
        "id": "8c4Z_j2nuEwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Type-2 Q-head ===\n",
        "def build_q_head_type2(x, action_size = 5, head_id=\"1\"):\n",
        "    # Advantage Stream\n",
        "    a = Dense(256, name=f\"a_{head_id}_dense1\")(x)\n",
        "    a = Dense(128, activation='relu', name=f\"a_{head_id}_dense2\")(a)\n",
        "    a = Dense(action_size, name=f\"a_{head_id}_out\")(a)\n",
        "\n",
        "    # Value Stream\n",
        "    v = Dense(256, activation='relu', name=f\"v_{head_id}_dense1\")(x)\n",
        "    v = Dense(128, name=f\"v_{head_id}_dense2\")(v)\n",
        "    v = Dropout(0.2)(v)\n",
        "    v = Dense(1, name=f\"v_{head_id}_out\")(v)\n",
        "\n",
        "    # Combine into Q-values\n",
        "    q = Lambda(dueling_q_output, name=f\"q_{head_id}\")([v, a])\n",
        "    return q"
      ],
      "metadata": {
        "id": "7iaEnmj-uJa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type-3 Q-Head"
      ],
      "metadata": {
        "id": "_ZkU32gPuU8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Type-3 Q-head ===\n",
        "def build_q_head_type3(x, action_size = 5, head_id=\"1\"):\n",
        "    # Advantage Stream\n",
        "    a = Dense(256, name=f\"a_{head_id}_dense1\")(x)\n",
        "    a = Dense(128, activation='relu', name=f\"a_{head_id}_dense2\")(a)\n",
        "    a = Dropout(0.2)(a)\n",
        "    a = Dense(action_size, name=f\"a_{head_id}_out\")(a)\n",
        "\n",
        "    # Value Stream\n",
        "    v = Dense(256, activation='relu', name=f\"v_{head_id}_dense1\")(x)\n",
        "    v = Dense(128, name=f\"v_{head_id}_dense2\")(v)\n",
        "    v = Dropout(0.2)(v)\n",
        "    v = Dense(1, name=f\"v_{head_id}_out\")(v)\n",
        "\n",
        "    # Combine into Q-values\n",
        "    q = Lambda(dueling_q_output, name=f\"q_{head_id}\")([v, a])\n",
        "    return q"
      ],
      "metadata": {
        "id": "Xxa3FThuuZx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type-4 Q-head"
      ],
      "metadata": {
        "id": "mi7wwjpYvjYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Type-4 Q-head ===\n",
        "def build_q_head_type4(x, action_size = 5, head_id=\"1\"):\n",
        "    # Advantage Stream\n",
        "    a = Dense(256, name=f\"a_{head_id}_dense1\")(x)\n",
        "    a = Dropout(0.2)(a)\n",
        "    a = Dense(128, activation='relu', name=f\"a_{head_id}_dense2\")(a)\n",
        "    a = Dense(action_size, name=f\"a_{head_id}_out\")(a)\n",
        "\n",
        "    # Value Stream\n",
        "    v = Dense(256, activation='relu', name=f\"v_{head_id}_dense1\")(x)\n",
        "    v = Dense(128, name=f\"v_{head_id}_dense2\")(v)\n",
        "    v = Dense(1, name=f\"v_{head_id}_out\")(v)\n",
        "\n",
        "    # Combine into Q-values\n",
        "    q = Lambda(dueling_q_output, name=f\"q_{head_id}\")([v, a])\n",
        "    return q"
      ],
      "metadata": {
        "id": "nP9ZNEXVvnz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type-5 Q-head"
      ],
      "metadata": {
        "id": "pTtVD1gOvxKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Type-5 Q-head ===\n",
        "def build_q_head_type5(x, action_size = 5, head_id=\"1\"):\n",
        "    # Advantage Stream\n",
        "    a = Dense(256, name=f\"a_{head_id}_dense1\")(x)\n",
        "    a = Dense(128, activation='relu', name=f\"a_{head_id}_dense2\")(a)\n",
        "    a = Dense(action_size, name=f\"a_{head_id}_out\")(a)\n",
        "\n",
        "    # Value Stream\n",
        "    v = Dense(256, activation='relu', name=f\"v_{head_id}_dense1\")(x)\n",
        "    v = Dropout(0.2)(v)\n",
        "    v = Dense(128, name=f\"v_{head_id}_dense2\")(v)\n",
        "    v = Dense(1, name=f\"v_{head_id}_out\")(v)\n",
        "\n",
        "    # Combine into Q-values\n",
        "    q = Lambda(dueling_q_output, name=f\"q_{head_id}\")([v, a])\n",
        "    return q"
      ],
      "metadata": {
        "id": "rkO0wi5avztX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Ensemble Heads"
      ],
      "metadata": {
        "id": "H2M0IHo2bJoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ensemble_heads(x, action_size, head_configs):\n",
        "    head_builders = {\n",
        "        \"type0\": build_q_head_type0,\n",
        "        \"type1\": build_q_head_type1,\n",
        "        \"type2\": build_q_head_type2,\n",
        "        \"type3\": build_q_head_type3,\n",
        "        \"type4\": build_q_head_type4,\n",
        "        \"type5\": build_q_head_type5,\n",
        "    }\n",
        "\n",
        "    q_heads = []\n",
        "    for i, head_type in enumerate(head_configs):\n",
        "        if head_type is None:\n",
        "            continue  # skip this head\n",
        "\n",
        "        head_id = str(i + 1)\n",
        "        if head_type == \"type0\":\n",
        "            q = build_q_head_type0(x, action_size, head_id)\n",
        "        elif head_type == \"type1\":\n",
        "            q = build_q_head_type1(x, action_size, head_id)\n",
        "        elif head_type == \"type2\":\n",
        "            q = build_q_head_type2(x, action_size, head_id)\n",
        "        elif head_type == \"type3\":\n",
        "            q = build_q_head_type3(x, action_size, head_id)\n",
        "        elif head_type == \"type4\":\n",
        "            q = build_q_head_type4(x, action_size, head_id)\n",
        "        elif head_type == \"type5\":\n",
        "            q = build_q_head_type5(x, action_size, head_id)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown head type: {head_type}\")\n",
        "\n",
        "        q_heads.append(q)\n",
        "\n",
        "    return q_heads"
      ],
      "metadata": {
        "id": "RMrmm_lVbNba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer Perceptron Builder"
      ],
      "metadata": {
        "id": "nnULlBircM6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dense_mlp(x, units_list, activation='relu', name_prefix=\"mlp\"):\n",
        "    for idx, units in enumerate(units_list):\n",
        "        if units is None or units == 0:\n",
        "            continue  # skip this layer\n",
        "        x = Dense(units, activation=activation, name=f\"{name_prefix}_dense{idx+1}\")(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "q9Gc2OgjcVhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš¦ Main Model"
      ],
      "metadata": {
        "id": "8YOj47qqHDFi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoE0FKeMmnmS"
      },
      "outputs": [],
      "source": [
        "# ==== Build Model ====\n",
        "# If you use stack frame, the input is (96, 96, 12)\n",
        "# If you don't use stack frame, the input is (96, 96, 3)\n",
        "def build_model (input_shape = (96, 96, 12), action_size = 5):\n",
        "  inputs = Input(shape = input_shape) # (96, 96, 12)\n",
        "\n",
        "  # Normalize pixel [0,255] â†’ [0,1]\n",
        "  x = tf.keras.layers.Rescaling(1./255)(inputs) # (96, 96, 12)\n",
        "\n",
        "  # ==== Gray ====\n",
        "  x = Conv2D(8, (1, 1), activation='relu')(x)  # (96, 96, 8)\n",
        "\n",
        "\n",
        "  # ==== Visual Cortex ====\n",
        "\n",
        "  # ==== Residual ====\n",
        "  residual0 = x # (96, 96, 8)\n",
        "\n",
        "  # ==== Contrast Enhancement ====\n",
        "\n",
        "  # ==== Recommended =====\n",
        "  x = ContrastEnhance(alpha=2.0)(x) # (96, 96, 8)\n",
        "  #x = ContrastEnhance(alpha=2.0)(x) # (96, 96, 8)\n",
        "\n",
        "  # ==== Sober Edge Layer ====\n",
        "  x = SobelEdgeLayer()(x) # (96, 96, 8)\n",
        "\n",
        "  x = Add()([x, residual0]) # (96, 96, 8)\n",
        "\n",
        "  # ==== Convolution Backbone  1====\n",
        "\n",
        "  # ==== Convolution Layers ====\n",
        "  x1 = Conv2D(16, (8, 8), strides=4, activation = None)(x) # (23, 23, 16)\n",
        "  x1 = ReLU()(x1) # You can try ReLU or LeakyReLU\n",
        "\n",
        "  # ==== Residual 1 ====\n",
        "\n",
        "  x1 = res_block(x1, 16) # (23, 23, 16)\n",
        "\n",
        "  # Input 23 â†’ (23 - 4)/2 + 1 = 10\n",
        "  x1 = Conv2D(32, (4, 4), strides=2, activation = None)(x1)\n",
        "  x1 = ReLU()(x1)\n",
        "\n",
        "  # ==== Residual 2 ====\n",
        "  x1 = res_block(x1, 32) # (10, 10, 32)\n",
        "\n",
        "  # Input 10 â†’ (10 - 3)/1 + 1 = 8\n",
        "  x1 = Conv2D(64, (3, 3), strides=1, activation = None)(x1)\n",
        "  x1 = ReLU()(x1)\n",
        "\n",
        "  # Input 8 â†’ (8 - 3)/1 + 1 = 6\n",
        "  x1 = Conv2D(128, (3, 3), strides=1, activation = None)(x1)\n",
        "  x1 = ReLU()(x1)\n",
        "\n",
        "  # ==== Flaten ====\n",
        "  x1 = Flatten()(x1)\n",
        "\n",
        "\n",
        "  # ==== Convolution Backbone  2====\n",
        "\n",
        "  # ==== Convolution Layers ====\n",
        "  x2 = Conv2D(16, (8, 8), strides=4, activation = None)(residual0) # (23, 23, 16)\n",
        "  x2 = ReLU()(x2) # You can try ReLU or LeakyReLU\n",
        "\n",
        "  # ==== Residual 1 ====\n",
        "\n",
        "  x2 = res_block(x2, 16) # (23, 23, 16)\n",
        "\n",
        "  # Input 23 â†’ (23 - 4)/2 + 1 = 10\n",
        "  x2 = Conv2D(32, (4, 4), strides=2, activation = None)(x2)\n",
        "  x2 = ReLU()(x2)\n",
        "\n",
        "  # ==== Residual 2 ====\n",
        "  x2 = res_block(x2, 32) # (10, 10, 32)\n",
        "\n",
        "  # Input 10 â†’ (10 - 3)/1 + 1 = 8\n",
        "  x2 = Conv2D(64, (3, 3), strides=1, activation = None)(x2)\n",
        "  x2 = ReLU()(x2)\n",
        "\n",
        "  # Input 8 â†’ (8 - 3)/1 + 1 = 6\n",
        "  x2 = Conv2D(128, (3, 3), strides=1, activation = None)(x2)\n",
        "  x2 = ReLU()(x2)\n",
        "\n",
        "  # ==== Flaten ====\n",
        "  x2 = Flatten()(x2)\n",
        "\n",
        "  # ==== Combine CNN 1 and 2 ====\n",
        "  # === Merge branches ===\n",
        "  merged = Concatenate()([x1, x2])\n",
        "\n",
        "\n",
        "  # ==== Deep Layers ====\n",
        "  # âœ… ==== Brain Gene Code ====\n",
        "  brain_gene = [512, 256, 128, 64, None]\n",
        "\n",
        "  x = build_dense_mlp(merged, units_list= brain_gene, activation='relu', name_prefix= 'brain')\n",
        "\n",
        "  # ==== Dueling DQN ==== Original Version ====\n",
        "  # Two head mechanism\n",
        "  # Advantage Stream\n",
        "  #a = Dense(256, activation= 'relu')(x)\n",
        "  #a = NoisyDense(128, sigma_init=0.05)(a)\n",
        "  #a = Dense(action_size)(a)\n",
        "\n",
        "\n",
        "  # Value Stream\n",
        "  #v = Dense(256, activation = 'relu')(x)\n",
        "  #v = Dense(128)(v)\n",
        "  #v = Dense(1)(v)\n",
        "\n",
        "  # ==== Ensemble Dueling DQN ====\n",
        "\n",
        "  # ==== Use Head Builder ====\n",
        "\n",
        "  # âœ… ==== Head Gene Code ===\n",
        "  head_gene = ['type1', 'type2', 'type3', None]\n",
        "\n",
        "  q_heads = build_ensemble_heads(x, action_size= 5, head_configs = head_gene)\n",
        "\n",
        "  # ==== Combine ====\n",
        "\n",
        "  q_mean = Average()(q_heads)\n",
        "\n",
        "  # ==== Output ====\n",
        "  # Q-values for each discrete action\n",
        "  # Output: q_values = [[12.3, 11.8, 9.5, 14.2, 13.0]]\n",
        "  # The q_values of each action\n",
        "  # i.e., Q(s, a = 0) = 12.3\n",
        "  # i.e., Q(s, a =1) = 11.8\n",
        "  #outputs = Dense(action_size, activation='linear', name = 'Q-values')(x)\n",
        "\n",
        "  # ==== Dueling DQN Output ====\n",
        "  # Q(s, a) = V(s) + (A(s, a) - mean(A(s, a')))\n",
        "  # V(s) -> (batch_size, 1)\n",
        "  # A(s, a) -> (batch_size, action_size)\n",
        "\n",
        "  # ==== Output ====\n",
        "  # Note: Remember add \"output_shape = (None, action_size)\"\n",
        "  #outputs = Lambda(dueling_q_output, output_shape=(action_size,))([v, a])\n",
        "\n",
        "  outputs = q_mean\n",
        "\n",
        "  # ==== Model Build ====\n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(optimizer=Adam(learning_rate=0.00025), loss='huber')\n",
        "\n",
        "  return model\n",
        "\n",
        "# ==== Car Racing parameters ====\n",
        "state_size = 96*96*12\n",
        "action_size = 5\n",
        "\n",
        "# ==== Structure ====\n",
        "test_model = build_model(input_shape = (96, 96, 12), action_size = 5)\n",
        "test_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y1LB_CzpPvs"
      },
      "source": [
        "# Implement the replay buffer (Memory)\n",
        "\n",
        "â¬‡ï¸ Memory\n",
        "\n",
        "For single step memory:\n",
        "\n",
        "It is just a note.\n",
        "\n",
        "\n",
        "```\n",
        "memory = deque(maxlen=10000)\n",
        "def remember(state, action, reward, next_state, done):\n",
        "    memory.append((state, action, reward, next_state, done))\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUlFLWFl7NsI"
      },
      "source": [
        "## SumTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P60EH-zl7Q30"
      },
      "outputs": [],
      "source": [
        "class SumTree:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)  # binary tree\n",
        "        self.data = np.zeros(capacity, dtype=object)  # actual data\n",
        "        self.write = 0\n",
        "        self.n_entries = 0\n",
        "\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "        self.tree[parent] += change\n",
        "\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(right, s - self.tree[left])\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    def add(self, priority, data):\n",
        "        idx = self.write + self.capacity - 1\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, priority)\n",
        "        #print(f\"Added to tree_idx={idx}, priority={priority}, data={data}\")\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "        self.n_entries = min(self.n_entries + 1, self.capacity)\n",
        "\n",
        "    def update(self, idx, priority):\n",
        "        change = priority - self.tree[idx]\n",
        "        self.tree[idx] = priority\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        data_idx = idx - self.capacity + 1\n",
        "        return (idx, self.tree[idx], self.data[data_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOYBX-eT7ya7"
      },
      "outputs": [],
      "source": [
        "# ==== Sum Tree Test Cell ====\n",
        "tree = SumTree(capacity=5)\n",
        "\n",
        "# Insert 5 data with different priority priority\n",
        "for i in range(5):\n",
        "    priority = (i + 1) * 10  # 10, 20, ..., 50\n",
        "    data = f\"data_{i}\"\n",
        "    tree.add(priority, data)\n",
        "    print(f\"âœ… Added {data} with priority {priority}\")\n",
        "\n",
        "print(\"\\nðŸŒ³ Tree total priority:\", tree.total())  # 10 + 20 + 30 + 40 + 50 = 150\n",
        "\n",
        "# Use get() to simulate 0 ~ total() sampling\n",
        "print(\"\\nðŸŽ¯ Sampling test:\")\n",
        "for s in [5, 15, 35, 75, 125]:\n",
        "    idx, p, data = tree.get(s)\n",
        "    print(f\"s={s:>3} â†’ index={idx}, priority={p}, data={data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PvzS27H9Yjq"
      },
      "outputs": [],
      "source": [
        "# ==== Test Sum Tree ====\n",
        "tree = SumTree(capacity=5)\n",
        "\n",
        "# Add 5 items\n",
        "for i in range(5):\n",
        "    tree.add(priority=(i + 1) * 10, data=f\"data_{i}\")  # Priorities: 10, 20, ..., 50\n",
        "\n",
        "print(f\"ðŸŒ³ Initial total: {tree.total()}\")  # Expect 150\n",
        "\n",
        "# === Test update ===\n",
        "# Let's update index 7 (data_3) to a new priority = 100\n",
        "tree_idx = 7\n",
        "tree.update(tree_idx, 100)\n",
        "\n",
        "# Check tree total\n",
        "print(f\"ðŸŒ³ After update total: {tree.total()}\")  # Expect: 150 - 40 + 100 = 210\n",
        "\n",
        "# === Sampling ===\n",
        "print(\"\\nðŸŽ¯ Sampling after update:\")\n",
        "for s in [5, 15, 45, 105, 175]:\n",
        "    idx, priority, data = tree.get(s)\n",
        "    print(f\"s={s:<3} â†’ index={idx}, priority={priority}, data={data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksDyXcR29rTs"
      },
      "source": [
        "##Prioritized Multiple Step Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1NS2vq6_Asl"
      },
      "outputs": [],
      "source": [
        "class PrioritizedMultiStepReplayBuffer:\n",
        "    def __init__(self, capacity=100000, n_step=5, gamma=0.99):\n",
        "        self.tree = SumTree(capacity)  # â† Replace deque\n",
        "        self.capacity = capacity\n",
        "        self.n_step = n_step\n",
        "        self.gamma = gamma\n",
        "        self.n_step_buffer = deque(maxlen=n_step)\n",
        "        self.max_priority = 1.0  # Initialize: Prevent all priority = 0\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "    # Add n-step buffer\n",
        "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        # If it reaches n step, process real transition\n",
        "        if len(self.n_step_buffer) == self.n_step:\n",
        "            s, a, _, _, _ = self.n_step_buffer[0]\n",
        "            R, s_n, done_n = self._compute_n_step_return()\n",
        "            transition = (s, a, R, s_n, done_n)\n",
        "            self.tree.add(self.max_priority, transition)\n",
        "            self.n_step_buffer.popleft()\n",
        "\n",
        "        # If episode is done, clear\n",
        "        if done:\n",
        "            while len(self.n_step_buffer) > 0:\n",
        "                s, a, _, _, _ = self.n_step_buffer[0]\n",
        "                R, s_n, done_n = self._compute_n_step_return()\n",
        "                transition = (s, a, R, s_n, done_n)\n",
        "                self.tree.add(self.max_priority, transition)\n",
        "                self.n_step_buffer.popleft()\n",
        "\n",
        "    def _compute_n_step_return(self):\n",
        "        R = 0\n",
        "        for idx, (_, _, r, _, d) in enumerate(self.n_step_buffer):\n",
        "            R += (self.gamma ** idx) * r\n",
        "            if d:\n",
        "                break\n",
        "        s_n = self.n_step_buffer[idx][3]\n",
        "        done_n = self.n_step_buffer[idx][4]\n",
        "        return R, s_n, done_n\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        indices = []\n",
        "        priorities = []\n",
        "        transitions = []\n",
        "\n",
        "        # Total priority\n",
        "        total = self.tree.total()\n",
        "        if total == 0:\n",
        "          raise ValueError(\"Cannot sample from an empty SumTree with total priority 0.\")\n",
        "\n",
        "        # Segment\n",
        "        segment = total / batch_size\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            s = np.random.uniform(a, b)  # Uniform sampling in [a, b]\n",
        "            idx, p, data = self.tree.get(s)\n",
        "            priorities.append(p)\n",
        "            transitions.append(data)\n",
        "            indices.append(idx)\n",
        "\n",
        "        # normalize IS weights\n",
        "        probs = np.array(priorities) / total\n",
        "        weights = (self.tree.n_entries * probs) ** (-beta)\n",
        "        weights /= weights.max()  # normalize\n",
        "\n",
        "        # Unpack batch\n",
        "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
        "\n",
        "        # Check your output\n",
        "        return (\n",
        "            np.array(states), # 1\n",
        "            np.array(actions), # 2\n",
        "            np.array(rewards, dtype=np.float32), # 3\n",
        "            np.array(next_states), # 4\n",
        "            np.array(dones, dtype=np.float32), # 5\n",
        "            np.array(indices), # 6\n",
        "            np.array(weights, dtype=np.float32) # 7\n",
        "        )\n",
        "    def __len__(self):\n",
        "      return self.tree.n_entries\n",
        "\n",
        "    def clear(self):\n",
        "      self.tree = SumTree(self.tree.capacity)  # Build a new SumTree\n",
        "      self.n_step_buffer.clear()               # Clear multi-step buffer\n",
        "\n",
        "    def update_priorities(self, indices, new_priorities):\n",
        "      for idx, priority in zip(indices, new_priorities):\n",
        "          epsilon = 1e-6\n",
        "          priority = np.clip(priority, epsilon, None) # ðŸ‘ˆ Prevent priority becomes 0\n",
        "          self.tree.update(idx, priority) # ðŸ‘ˆ Sent back to SumTree\n",
        "          self.max_priority = max(self.max_priority, priority) # ðŸ‘ˆ Remain max for new transition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Step Replay Buffer"
      ],
      "metadata": {
        "id": "4aH5ulbn5PSY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwC0vSCdevWi"
      },
      "outputs": [],
      "source": [
        "class MultiStepReplayBuffer:\n",
        "    def __init__(self, capacity=100000, n_step=5, gamma=0.99):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.n_step = n_step\n",
        "        self.gamma = gamma\n",
        "        self.n_step_buffer = deque(maxlen=n_step)\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
        "        if len(self.n_step_buffer) == self.n_step:\n",
        "            s, a, _, _, _ = self.n_step_buffer[0]\n",
        "            R, s_n, done_n = self._compute_n_step_return()\n",
        "            self.buffer.append((s, a, R, s_n, done_n))\n",
        "        if done:\n",
        "            while len(self.n_step_buffer) > 0:\n",
        "                s, a, _, _, _ = self.n_step_buffer[0]\n",
        "                R, s_n, done_n = self._compute_n_step_return()\n",
        "                self.buffer.append((s, a, R, s_n, done_n))\n",
        "                self.n_step_buffer.popleft()\n",
        "\n",
        "    def _compute_n_step_return(self):\n",
        "        R = 0\n",
        "        for idx, (_, _, r, _, d) in enumerate(self.n_step_buffer):\n",
        "            R += (self.gamma ** idx) * r\n",
        "            if d: break\n",
        "        s_n = self.n_step_buffer[idx][3]\n",
        "        done_n = self.n_step_buffer[idx][4]\n",
        "        return R, s_n, done_n\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        import random\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        # Check your output\n",
        "        return (\n",
        "                np.array(states),  # 1\n",
        "                np.array(actions),  # 2\n",
        "                np.array(rewards),  # 3\n",
        "                np.array(next_states), # 4\n",
        "                np.array(dones) # 5\n",
        "              )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "        self.n_step_buffer.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXlXb9zODss5"
      },
      "outputs": [],
      "source": [
        "# ==== Test Buffer ====\n",
        "# ====  Dummy Data ====\n",
        "state = np.zeros((96, 96, 12), dtype=np.uint8)\n",
        "next_state = np.ones((96, 96, 12), dtype=np.uint8)\n",
        "\n",
        "# Initialize buffer\n",
        "buffer = PrioritizedMultiStepReplayBuffer(capacity=10, n_step=3, gamma=0.99)\n",
        "\n",
        "# ==== TEST 1: Sent n-step transitions ====\n",
        "for i in range(5):\n",
        "    s = state + i  # Change state\n",
        "    ns = next_state + i\n",
        "    buffer.store(s, i % 5, 1.0 + i, ns, done=False)\n",
        "buffer.store(s, 0, 0.5, ns, done=True)  # suppose episode ends\n",
        "\n",
        "print(f\"âœ… Buffer length after insert: {len(buffer)}\")\n",
        "print(f\"âœ… Tree total priority: {buffer.tree.total():.2f}\")\n",
        "\n",
        "# ==== TEST 2: sample transitions ====\n",
        "sampled = buffer.sample(batch_size=3)\n",
        "states, actions, rewards, next_states, dones, indices, weights = sampled\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Sampled actions: {actions}\")\n",
        "print(f\"ðŸŽ¯ Sampled indices: {indices}\")\n",
        "print(f\"ðŸŽ¯ Importance weights: {weights.round(4)}\")\n",
        "\n",
        "# ==== TEST 3: update priority ====\n",
        "new_priorities = np.random.uniform(0.1, 5.0, size=3)\n",
        "print(f\"\\nðŸ“¦ New priorities: {new_priorities.round(2)}\")\n",
        "buffer.update_priorities(indices, new_priorities)\n",
        "\n",
        "print(f\"âœ… Updated tree total priority: {buffer.tree.total():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZMj9b9_ql0E"
      },
      "source": [
        "# Epsilon Greedy Policy (Act)\n",
        "\n",
        "âž¡ï¸ epsilon control\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iqlW7pWp62T"
      },
      "outputs": [],
      "source": [
        "# ==== Epsilon-greedy Parameters ====\n",
        "# exploration rate\n",
        "epsilon = 1.0\n",
        "# minimum exploration rate\n",
        "epsilon_min = 0.01\n",
        "# dacay rate for epsilon after each episode\n",
        "epsilon_decay =100000 # 10000\n",
        "\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.05\n",
        "\n",
        "total_steps = 0\n",
        "\n",
        "# ==== Action ====\n",
        "# act(): chooses an action based on the epsilon-greedy policy.\n",
        "# ==== Action Selection ====\n",
        "def act(state, use_epsilon):\n",
        "    global epsilon\n",
        "\n",
        "    if use_epsilon:\n",
        "      if np.random.rand() < epsilon:\n",
        "          return np.random.choice(action_size)  # Randomly select an action\n",
        "\n",
        "    q_values = model.predict(state[np.newaxis, ...], verbose=0)\n",
        "    return np.argmax(q_values[0])\n",
        "\n",
        "# ==== Decay epsilon ====\n",
        "def decay_epsilon(total_steps, episode):\n",
        "    global epsilon\n",
        "\n",
        "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * total_steps / epsilon_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q-U177BgLkN"
      },
      "source": [
        "# Double DQN Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rly4QOB4gHrZ"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "\n",
        "# ==== Training Step ====\n",
        "def train_step(states, target_qs, weights):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_pred = model(states, training=True)\n",
        "        elementwise_loss = tf.keras.losses.Huber(reduction='none')(target_qs, q_pred)\n",
        "        loss = tf.reduce_mean(elementwise_loss * weights)  # â† IS weighting\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "# ==== Double Replay ====\n",
        "def double_replay(batch_size, episode):\n",
        "  global epsilon, steps\n",
        "\n",
        "  # === Safe Sample ===\n",
        "  sampled = memory.sample(batch_size)\n",
        "\n",
        "  if isinstance(memory, PrioritizedMultiStepReplayBuffer):\n",
        "      states, actions, rewards, next_states, dones, indices, weights = sampled\n",
        "  else:\n",
        "      states, actions, rewards, next_states, dones = sampled\n",
        "      indices = None\n",
        "      weights = np.ones_like(rewards)\n",
        "\n",
        "  # ==== Debug shape ====\n",
        "  if states.shape != (batch_size, 96, 96, 12):\n",
        "      print(f\"âš ï¸ [Episode {episode}] Sample shape mismatch: {states.shape}\")\n",
        "      return\n",
        "\n",
        "  # ==== Check ====\n",
        "  if states.shape != (batch_size, 96, 96, 12):\n",
        "    print(f\"âŒ Shape mismatch: got {states.shape}, skipping training\")\n",
        "    return\n",
        "\n",
        "  # ==== Double DQN target ====\n",
        "  next_q_main = model.predict(next_states, verbose=0)\n",
        "  # Predict next Q-values using main model (for action selection)\n",
        "  next_actions = np.argmax(next_q_main, axis=1)\n",
        "  # Predict next Q-values using target model (for evaluation)\n",
        "  next_q_target = target_model.predict(next_states, verbose=0)\n",
        "\n",
        "  # ==== Get current Q values ====\n",
        "  q_values = model.predict(states, verbose=0)\n",
        "\n",
        "  # ==== TD error ====\n",
        "  td_errors = []\n",
        "\n",
        "  for i in range(batch_size):\n",
        "      target = rewards[i] if dones[i] else rewards[i] + gamma * next_q_target[i][next_actions[i]]\n",
        "      td_error = abs(target - q_values[i][actions[i]])  # â† TD error\n",
        "      td_errors.append(td_error)\n",
        "      q_values[i][actions[i]] = target\n",
        "\n",
        "  # Train on updated Q-values\n",
        "  # Call the tf.function-wrapped training step\n",
        "  # ==== Training Step ====\n",
        "  train_step(\n",
        "    tf.convert_to_tensor(states, dtype=tf.float32),\n",
        "    tf.convert_to_tensor(q_values, dtype=tf.float32),\n",
        "    tf.convert_to_tensor(weights[:, None], dtype=tf.float32),  # ðŸ‘ˆ shape=(batch_size, 1)\n",
        "  )\n",
        "\n",
        "  # ==== Update Priorities ====\n",
        "  if indices is not None:\n",
        "    memory.update_priorities(indices, td_errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-hpX2zd4VBq"
      },
      "source": [
        "# Training Loop Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU7AHs0079mW"
      },
      "source": [
        "### Frame Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP82-tBy57GZ"
      },
      "outputs": [],
      "source": [
        "# ==== Frame Stack ====\n",
        "# Initialize buffer\n",
        "NUM_FRAMES = 4\n",
        "frame_buffer = deque(maxlen=NUM_FRAMES)\n",
        "\n",
        "def preprocess_frame(frame, size=(96, 96), grayscale=False):\n",
        "    if grayscale:\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, size)\n",
        "        frame = np.expand_dims(frame, axis=-1)  # (96,96,1)\n",
        "    else:\n",
        "        frame = cv2.resize(frame, size)  # RGB\n",
        "    return frame.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECbjVDMW3IMl"
      },
      "source": [
        "### Shaping reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS6k-wfp2cBU"
      },
      "outputs": [],
      "source": [
        "def reward_shaping(action, reward, next_state=None, info=None):\n",
        "  shaped_reward = float(reward)\n",
        "\n",
        "  # Punish stay\n",
        "  if action == 0:\n",
        "    shaped_reward -= 0.1  # Punish not moving\n",
        "  elif action == 3:  # Gas\n",
        "      shaped_reward += 0.00\n",
        "  elif action in [1, 2]:  # Tile\n",
        "      shaped_reward += 0.00\n",
        "  elif action == 4:  # Brake\n",
        "      shaped_reward -= 0.00\n",
        "\n",
        "  return shaped_reward\n",
        "\n",
        "# ==== How to Use ====\n",
        "# In training loop:\n",
        "# for each episode:\n",
        "# reward = shaped_reward(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display"
      ],
      "metadata": {
        "id": "9-zRUWSlvqYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5QHWRzSl2AX"
      },
      "outputs": [],
      "source": [
        "# ==== Display ====\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_training_progress(rewards, smoothing=0.9):\n",
        "    # ==== Smooth ====\n",
        "    def smooth(data, weight):\n",
        "        smoothed = []\n",
        "        last = data[0]\n",
        "        for point in data:\n",
        "            smoothed_val = last * weight + (1 - weight) * point\n",
        "            smoothed.append(smoothed_val)\n",
        "            last = smoothed_val\n",
        "        return smoothed\n",
        "\n",
        "      # ==== Moving Std====\n",
        "    def moving_std(data, window=20):\n",
        "        return [np.std(data[max(0, i - window):i + 1]) for i in range(len(data))]\n",
        "\n",
        "\n",
        "    smoothed_rewards = smooth(rewards, smoothing)\n",
        "\n",
        "    std_rewards = moving_std(rewards)\n",
        "    upper = np.array(smoothed_rewards) + np.array(std_rewards)\n",
        "    lower = np.array(smoothed_rewards) - np.array(std_rewards)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.style.use(\"dark_background\")  # Dark background\n",
        "\n",
        "    x = np.arange(len(rewards))\n",
        "    plt.plot(x, rewards, color=\"#00FFAA\", alpha=0.3, label='Raw Reward')  # Original raw reward\n",
        "\n",
        "    smoothed = np.array(smoothed_rewards)\n",
        "    x = np.arange(len(smoothed))\n",
        "\n",
        "    # Separate into two arrays\n",
        "    positive = np.where(smoothed > 0, smoothed, 0)\n",
        "    negative = np.where(smoothed <= 0, smoothed, 0)\n",
        "\n",
        "    # Color\n",
        "    plt.fill_between(x, positive, color=\"red\", alpha=0.5, label='Positive Reward')\n",
        "    plt.fill_between(x, negative, color=\"green\", alpha=0.5, label='Negative Reward')\n",
        "\n",
        "    # ==== Green Line ====\n",
        "    plt.plot(x, smoothed_rewards, color=\"#00FFFF\", linewidth=2.0, label='Smoothed Reward')\n",
        "\n",
        "    plt.fill_between(x, smoothed_rewards, color=\"#00FFFF\", alpha=0.1)\n",
        "\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Training Progress â€” Reward Curve (Inspired by Crypto)')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # ==== Mark Max ====\n",
        "    max_idx = np.argmax(rewards)\n",
        "    plt.scatter([max_idx], [rewards[max_idx]], color='red', label='ðŸ“ˆ ATH', zorder=5)\n",
        "    plt.text(max_idx, rewards[max_idx]+5, f\"Max: {rewards[max_idx]:.2f}\", color='red')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Test Plot ====\n",
        "import numpy as np\n",
        "\n",
        "# ==== Test Reward sequence ====\n",
        "np.random.seed(42)  # For reproduce\n",
        "episodes = 200\n",
        "base_trend = np.concatenate([\n",
        "    np.linspace(-100, 600, 100),     # Early stage\n",
        "    np.linspace(600, -500, 50),       # Middle Stage\n",
        "    np.linspace(100, 900, 50)        # Final Stage\n",
        "])          # Slowly increase\n",
        "\n",
        "# ==== High fluctuation ====\n",
        "fluctuations = np.random.normal(0, 150, size=episodes)\n",
        "rewards = base_trend + fluctuations\n",
        "\n",
        "# ==== Call function ====\n",
        "plot_training_progress(rewards)"
      ],
      "metadata": {
        "id": "l3xYZd-OjoGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuxQFCwEFgNZ"
      },
      "source": [
        "## ðŸš¦Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IaKFU3U-ONB"
      },
      "outputs": [],
      "source": [
        "# ==== Test Frame Stack by Me ====\n",
        "from datetime import datetime\n",
        "\n",
        "global memory\n",
        "\n",
        "# ==== Build Target Network ====\n",
        "model = build_model(input_shape = (96, 96, 12), action_size = 5)\n",
        "target_model = build_model(input_shape=(96, 96, 12), action_size=5)\n",
        "target_model.set_weights(model.get_weights())\n",
        "\n",
        "\n",
        "\n",
        "# ==== Training Parameters ====\n",
        "episodes = 3000 # More is good\n",
        "batch_size = 128 # Minibatch size\n",
        "gamma = 0.95 # Discount rate (~1 future is import)\n",
        "max_step = 1000 # Max step per eposide\n",
        "best_score = -np.inf\n",
        "\n",
        "# ==== Before loop ====\n",
        "steps = 0  # Initialize the steps\n",
        "\n",
        "# ==== Record ====\n",
        "rewards_per_episode = []\n",
        "\n",
        "\n",
        "# ==== Initial memory type ====\n",
        "memory = MultiStepReplayBuffer(capacity=100000, n_step=5, gamma=0.99)\n",
        "\n",
        "# ==== Track whether switched ====\n",
        "memory_switched = False\n",
        "\n",
        "# ==== Clear memory to ensure consistent state shapes ====\n",
        "# This prevents the ValueError if memory contains states from a non-stacked training run.\n",
        "memory.clear()\n",
        "\n",
        "# ===================\n",
        "# ==== Training Loop ====\n",
        "for episode in range(episodes):\n",
        "\n",
        "  # ==== Switching Exploration Strategy ====\n",
        "\n",
        "  use_epsilon = True\n",
        "\n",
        "  # ==== Initialize Environment ====\n",
        "  env = gym.make(\"CarRacing-v3\",\n",
        "               render_mode= None,\n",
        "               lap_complete_percent=0.95,\n",
        "               domain_randomize=True,\n",
        "               continuous=False)\n",
        "\n",
        "  # ==== Action History ====\n",
        "  action_history = []\n",
        "\n",
        "  # ==== Reset Environment ====\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  # ==== Switch to prioritized buffer =====\n",
        "  # Dynamic Switching Memory\n",
        "  if episode == 500 and not memory_switched:\n",
        "      new_memory = PrioritizedMultiStepReplayBuffer(capacity=100000, n_step=5, gamma=0.99)\n",
        "\n",
        "      # Copy old data over (optional)\n",
        "      for item in memory.buffer:\n",
        "          new_memory.tree.add(1.0, item)  # set default priority\n",
        "      memory = new_memory\n",
        "      memory_switched = True\n",
        "      print(\"ðŸ§  Switched to Prioritized Replay Buffer.\")\n",
        "\n",
        "  # ==== No-op  ====\n",
        "  for _ in range(50):\n",
        "      state, _, terminated, truncated, _ = env.step(0)\n",
        "      if terminated or truncated:\n",
        "          break\n",
        "\n",
        "  # ==== Initialize stacking: RGB version ====\n",
        "  # Preprocess after no-op\n",
        "  # ==== Preprocess  ====\n",
        "  state = preprocess_frame(state, grayscale=False)\n",
        "  frame_buffer = deque([state] * 4, maxlen=4)\n",
        "  # ==== Stack State ====\n",
        "  stacked_state = np.concatenate(list(frame_buffer), axis=-1)\n",
        "\n",
        "  # ==== Episode Reward Initialize ====\n",
        "  episode_reward =0.0\n",
        "\n",
        "  # ==== Logic ====\n",
        "  # state â†’ select action â†’ step in env â†’ get next_state, reward â†’ modify â†’ store in memory\n",
        "\n",
        "  for time in range(max_step):\n",
        "    steps += 1  # Ensure epsilon dacay\n",
        "    # Choose action using epsilon-greedy policy\n",
        "    # act() â†’ strategy\n",
        "    # Use stacked state\n",
        "\n",
        "    # ==== Force Forward ====\n",
        "    # ==== NEW ====\n",
        "    if episode < 5 and time < 100:\n",
        "      # mode 1 (I recommend this one)\n",
        "      action = np.random.choice([1, 2, 3])\n",
        "\n",
        "      # mode 2\n",
        "      # action = np.random.choice([1, 2, 3, 4])\n",
        "    else:\n",
        "      action = act(stacked_state, use_epsilon)\n",
        "\n",
        "    action_history.append(action)\n",
        "\n",
        "    # ===== Take action ====\n",
        "    total_reward = 0\n",
        "\n",
        "    # ==== Frame Skipping ====\n",
        "    for _ in range(4):  # frame skip æ¬¡æ•¸\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward = total_reward + reward\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    # ==== Handle Next Step ====\n",
        "    if isinstance(next_state, tuple):\n",
        "      next_state = next_state[0]\n",
        "\n",
        "    next_state = preprocess_frame(next_state, grayscale=False)\n",
        "\n",
        "    # ==== Update Stack ====\n",
        "    frame_buffer.append(next_state)\n",
        "    stacked_next_state = np.concatenate(list(frame_buffer), axis=-1)\n",
        "\n",
        "    # ==== Reward Shaping ====\n",
        "    reward = reward_shaping(action, total_reward, stacked_next_state, None)\n",
        "\n",
        "    # ==== Remember ====\n",
        "    # Store experience in memory\n",
        "    # remember â†’ memorize (replay buffer)\n",
        "    # remember(stacked_state, action, reward, stacked_next_state, done)\n",
        "\n",
        "    # ==== Memorize ====\n",
        "    memory.store(stacked_state, action, reward, stacked_next_state, done)\n",
        "\n",
        "    # ==== Update state ====\n",
        "    stacked_state = stacked_next_state\n",
        "\n",
        "    # ==== Record reward ====\n",
        "    episode_reward = episode_reward + float(reward)\n",
        "\n",
        "    # If episode ends\n",
        "    if done:\n",
        "      rewards_per_episode.append(episode_reward)\n",
        "\n",
        "      # ====  Action Entropy ====\n",
        "      action_counts = np.array([action_history.count(a) for a in range(action_size)])\n",
        "\n",
        "      # Prevent log(0)\n",
        "      action_probs = action_counts / np.sum(action_counts) + 1e-8\n",
        "\n",
        "      action_entropy = scipy.stats.entropy(action_probs, base=2)\n",
        "\n",
        "      # ==== Plot =====\n",
        "\n",
        "      plot_training_progress(rewards_per_episode)\n",
        "\n",
        "      # ==== Print ====\n",
        "      print(f\"ðŸŽ¬ Episode {episode+1} | {datetime.now().strftime('%H:%M:%S')} | Reward: {episode_reward:.2f} | Epsilon: {epsilon:.4f} | Domain Randomization ={True} | Entropy: {action_entropy:.2f} bits\")\n",
        "      break\n",
        "\n",
        "  # ==== Decay Epsilon ====\n",
        "  decay_epsilon(steps, episode)\n",
        "\n",
        "  # ==== Train the model using replay memory ====\n",
        "\n",
        "  replay_start = 5000  # Tunable\n",
        "  if len(memory) > replay_start and len(memory) > batch_size:\n",
        "    double_replay(batch_size, episode=episode)\n",
        "\n",
        "  # ==== Reset Target Network ====\n",
        "  if episode % 10 == 0:\n",
        "    target_model.set_weights(model.get_weights())\n",
        "\n",
        "  # ==== Update Best Score ====\n",
        "  if episode_reward > best_score:\n",
        "    best_score = episode_reward\n",
        "    model.save('best_model.keras')\n",
        "    print(f\"âœ… New Best Score {best_score}\")\n",
        "\n",
        "  print(\"Top actions in this episode:\", Counter(action_history).most_common())\n",
        "  print(\"======================================\")\n",
        "\n",
        "  # ==== Close Env ====\n",
        "  env.close()\n",
        "\n",
        "\n",
        "print(f\"Training complete after {episodes} episodes.\")\n",
        "print(f\"Best model score: {best_score:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation (Randomize = True)"
      ],
      "metadata": {
        "id": "DJxvwv5VZnau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQLFqZyrJBll"
      },
      "outputs": [],
      "source": [
        "# Re-initialize the environment for evaluation\n",
        "# Use the same parameters as the training environment\n",
        "\n",
        "# ==== GIF file ====\n",
        "output_dir = \"test_real_gifs\"  # Name your file\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create it\n",
        "\n",
        "# ==== Evaluation ====\n",
        "evaluation_episodes = 100 # Number of evaluation\n",
        "score = []\n",
        "\n",
        "# ==== Initialize Environment ====\n",
        "env = gym.make(\"CarRacing-v3\",\n",
        "              render_mode= 'rgb_array',\n",
        "              lap_complete_percent=0.95,\n",
        "              domain_randomize=True,\n",
        "              continuous=False)\n",
        "\n",
        "\n",
        "\n",
        "for episode in range(evaluation_episodes):\n",
        "  # ==== Make GIF ====\n",
        "  gif_frames = []\n",
        "\n",
        "  obs, _ = env.reset()\n",
        "  frame = preprocess_frame(obs)\n",
        "\n",
        "  # Initialize frame buffer\n",
        "  frame_buffer = deque([frame]*NUM_FRAMES, maxlen=NUM_FRAMES)\n",
        "  stacked_state = np.concatenate(list(frame_buffer), axis=-1)  # shape: (96, 96, 12) if RGB\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "\n",
        "  for time in range(max_step):\n",
        "    gif_frames.append(env.render())\n",
        "\n",
        "    # Step 1: Select action\n",
        "    q_values = model(stacked_state[np.newaxis, ...], training=False)[0]\n",
        "    action = np.argmax(q_values)\n",
        "\n",
        "    # Step 2: Interaction with env.\n",
        "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "    # Step 3: Preprocess frame\n",
        "    next_frame = preprocess_frame(next_obs)\n",
        "\n",
        "    # Step 4: Update frame buffer + stacked_state\n",
        "    frame_buffer.append(next_frame)\n",
        "    stacked_state = np.concatenate(list(frame_buffer), axis=-1)\n",
        "\n",
        "    # Step 5: Update reward\n",
        "    total_reward += float(reward)\n",
        "\n",
        "    # Step 6: Check if done\n",
        "    done = terminated or truncated\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "  score.append(total_reward)\n",
        "  print(f\"Episode: {episode+1}/{evaluation_episodes}, Score: {total_reward:.2f}\")\n",
        "  gif_path = os.path.join(output_dir, f\"episode_{episode+1:03d}-{total_reward:.0f}.gif\")\n",
        "  imageio.mimsave(gif_path, gif_frames, fps=30)\n",
        "\n",
        "env.close()\n",
        "\n",
        "# ==== Scores ====\n",
        "average_score = np.mean(score)\n",
        "max_score = np.max(score)\n",
        "min_score = np.min(score)\n",
        "std_score = np.std(score)\n",
        "\n",
        "print(f'Evaluation complete after {evaluation_episodes} Episodes')\n",
        "print(f\"Average Score: {average_score:.2f}\")\n",
        "print(f\"Max Score: {max_score:.2f}\")\n",
        "print(f\"Min Score: {min_score:.2f}\")\n",
        "print(f\"Std Score: {std_score:.2f}\")\n",
        "\n",
        "# ==== Visualization ====\n",
        "def smooth(data, weight=0.9):\n",
        "    smoothed = []\n",
        "    last = data[0]\n",
        "    for point in data:\n",
        "        smoothed_val = last * weight + (1 - weight) * point\n",
        "        smoothed.append(smoothed_val)\n",
        "        last = smoothed_val\n",
        "    return np.array(smoothed)\n",
        "\n",
        "smoothed = smooth(score)\n",
        "std = np.std(score)\n",
        "\n",
        "\n",
        "# ==== Use White Background ====\n",
        "plt.rcParams['axes.facecolor'] = 'black'\n",
        "plt.rcParams['figure.facecolor'] = 'black'\n",
        "# ==== Plot ====\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(score))\n",
        "\n",
        "plt.plot(x, score, label='Raw Reward', alpha=0.3)\n",
        "\n",
        "plt.plot(x, smoothed, label='Smoothed Reward', linewidth=2)\n",
        "\n",
        "# ==== Std Band ====\n",
        "\n",
        "plt.fill_between(x, smoothed - std, smoothed + std, alpha=0.2, color='skyblue', label='Â±1 Std')\n",
        "\n",
        "# ==== Baseline ====\n",
        "plt.axhline(y=500, color='red', linestyle='--', label='Pass Threshold')\n",
        "\n",
        "# ==== Title and label ====\n",
        "plt.title('Evaluation Reward Trend with Std Band')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš¦ Evolution"
      ],
      "metadata": {
        "id": "Xh3kiplwojQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "vQVQZwSgq9uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Build Model ====\n",
        "# If you use stack frame, the input is (96, 96, 12)\n",
        "# If you don't use stack frame, the input is (96, 96, 3)\n",
        "\n",
        "visual_gene = [2, 0, 2.0, False]\n",
        "brain_gene = [512, 256, 128, 64, 0, 0]\n",
        "head_gene = ['type1', 'type2', 'type3']\n",
        "\n",
        "# ==== Build Evolution Agent ====\n",
        "def build_evo_agent (\n",
        "                     visual_gene = visual_gene,\n",
        "                     brain_gene = brain_gene,\n",
        "                     head_gene = head_gene):\n",
        "\n",
        "  input_shape = (96, 96, 12)\n",
        "  action_size = 5\n",
        "  inputs = Input(shape = input_shape) # (96, 96, 12)\n",
        "\n",
        "  # Normalize pixel [0,255] â†’ [0,1]\n",
        "  x = tf.keras.layers.Rescaling(1./255)(inputs) # (96, 96, 12)\n",
        "\n",
        "  # ==== Gray ====\n",
        "  x = Conv2D(8, (1, 1), activation='relu')(x)  # (96, 96, 8)\n",
        "\n",
        "\n",
        "  # ==== Visual Cortex ====\n",
        "  # ==== Experimental ====\n",
        "\n",
        "  # ==== Residual ====\n",
        "  #residual0 = x # (96, 96, 8)\n",
        "\n",
        "  # ==== Contrast Enhancement ====\n",
        "  # âœ… ==== Visual Gene Code ====\n",
        "  # [2, 0, 2.0, False]\n",
        "  # [Contrast Enhance, Sobel Layer, alpha, Residual]\n",
        "  # visual_gene = [2, 0, 2.0, False]\n",
        "  x = VisualEnhanceBlock(visual_gene)(x) # (96, 96, 8)\n",
        "\n",
        "  # ==== Recommended =====\n",
        "  #x = ContrastEnhance(alpha=2.0)(x) # (96, 96, 8)\n",
        "  #x = ContrastEnhance(alpha=2.0)(x) # (96, 96, 8)\n",
        "\n",
        "  # ==== Sober Edge Layer ====\n",
        "  #x = SobelEdgeLayer()(x) # (96, 96, 8)\n",
        "\n",
        "  #x = Add()([x, residual0]) # (96, 96, 8)\n",
        "\n",
        "  # ==== Convolution Backbone ====\n",
        "\n",
        "  # ==== Convolution Layers ====\n",
        "  x = Conv2D(16, (8, 8), strides=4, activation = None)(x) # (23, 23, 16)\n",
        "  x = ReLU()(x) # You can try ReLU or LeakyReLU\n",
        "\n",
        "  # ==== Residual 1 ====\n",
        "\n",
        "  x = res_block(x, 16) # (23, 23, 16)\n",
        "\n",
        "  # Input 23 â†’ (23 - 4)/2 + 1 = 10\n",
        "  x = Conv2D(32, (4, 4), strides=2, activation = None)(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  # ==== Residual 2 ====\n",
        "  x = res_block(x, 32) # (10, 10, 32)\n",
        "\n",
        "  # Input 10 â†’ (10 - 3)/1 + 1 = 8\n",
        "  x = Conv2D(64, (3, 3), strides=1, activation = None)(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  # Input 8 â†’ (8 - 3)/1 + 1 = 6\n",
        "  x = Conv2D(128, (3, 3), strides=1, activation = None)(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  # ==== Flaten ====\n",
        "  x = Flatten()(x)\n",
        "  #x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "  # ==== Deep Layers ====\n",
        "  # âœ… ==== Brain Gene Code ====\n",
        "  #brain_gene = [512, 256, 128, 64, None]\n",
        "\n",
        "  x = build_dense_mlp(x, units_list= brain_gene, activation='relu', name_prefix= 'brain')\n",
        "\n",
        "  # ==== Dueling DQN =====\n",
        "  # Two head mechanism\n",
        "  # Advantage Stream\n",
        "  #a = Dense(256, activation= 'relu')(x)\n",
        "  #a = NoisyDense(128, sigma_init=0.05)(a)\n",
        "  #a = Dense(action_size)(a)\n",
        "\n",
        "\n",
        "  # Value Stream\n",
        "  #v = Dense(256, activation = 'relu')(x)\n",
        "  #v = Dense(128)(v)\n",
        "  #v = Dense(1)(v)\n",
        "\n",
        "  # ==== Ensemble Dueling DQN ====\n",
        "\n",
        "  # ==== Use Head Builder ====\n",
        "\n",
        "  # âœ… ==== Head Gene Code ===\n",
        "  #head_gene = ['type1', 'type2', 'type3', None]\n",
        "\n",
        "  q_heads = build_ensemble_heads(x, action_size= 5, head_configs = head_gene)\n",
        "\n",
        "  # ==== Combine ====\n",
        "\n",
        "  q_mean = Average()(q_heads)\n",
        "\n",
        "  # ==== Output ====\n",
        "  # Q-values for each discrete action\n",
        "  # Output: q_values = [[12.3, 11.8, 9.5, 14.2, 13.0]]\n",
        "  # The q_values of each action\n",
        "  # i.e., Q(s, a = 0) = 12.3\n",
        "  # i.e., Q(s, a =1) = 11.8\n",
        "  #outputs = Dense(action_size, activation='linear', name = 'Q-values')(x)\n",
        "\n",
        "  # ==== Dueling DQN Output ====\n",
        "  # Q(s, a) = V(s) + (A(s, a) - mean(A(s, a')))\n",
        "  # V(s) -> (batch_size, 1)\n",
        "  # A(s, a) -> (batch_size, action_size)\n",
        "\n",
        "  # ==== Output ====\n",
        "  # Note: Remember add \"output_shape = (None, action_size)\"\n",
        "  #outputs = Lambda(dueling_q_output, output_shape=(action_size,))([v, a])\n",
        "\n",
        "  outputs = q_mean\n",
        "\n",
        "  # ==== Model Build ====\n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(optimizer=Adam(learning_rate=0.00025), loss='huber')\n",
        "\n",
        "  return model\n",
        "\n",
        "# ==== Car Racing parameters ====\n",
        "state_size = 96*96*12\n",
        "action_size = 5\n",
        "\n",
        "# ==== Structure ====\n",
        "test_model = build_evo_agent(visual_gene, brain_gene, head_gene)\n",
        "test_model.summary()"
      ],
      "metadata": {
        "id": "f3rBt9qmolb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gene Generator"
      ],
      "metadata": {
        "id": "RfLZDDDGreGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_gene():\n",
        "    visual_gene = [\n",
        "        np.random.randint(0, 3),              # num_contrast\n",
        "        np.random.randint(0, 3),              # num_sobel\n",
        "        round(np.random.uniform(1.0, 3.0), 2),          # alpha\n",
        "        np.random.choice([True, False])       # use_residual\n",
        "    ]\n",
        "    brain_gene = random.choices([512, 256, 128, 64, None], k=np.random.randint(3,6))\n",
        "    head_gene = random.choices(['type1', 'type2', 'type3', 'type4', None], k=np.random.randint(2,6))\n",
        "    return {\n",
        "        \"visual\": visual_gene,\n",
        "        \"brain\": brain_gene,\n",
        "        \"head\": head_gene\n",
        "    }"
      ],
      "metadata": {
        "id": "reT6aWalrg0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def name_gene(gene):\n",
        "  visual_gene = gene[\"visual\"]\n",
        "  brain_gene = gene[\"brain\"]\n",
        "  head_gene = gene[\"head\"]\n",
        "  visual_code = f\"V{visual_gene[0]}{visual_gene[1]}\"\n",
        "  brain_code = f\"B{([i for i in brain_gene if i])}\"\n",
        "  head_code = f\"H{''.join([str(i[-1]) for i in head_gene if i])}\"\n",
        "\n",
        "  gene_name = f\"{visual_code}-{brain_code}-{head_code}\"\n",
        "  return gene_name"
      ],
      "metadata": {
        "id": "20sOwXYX-IYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== TEST ====\n",
        "\n",
        "# ==== Test Gene ====\n",
        "test_gene = random_gene()\n",
        "\n",
        "print(test_gene)\n",
        "print(name_gene(test_gene))\n",
        "\n",
        "# ==== USE test gene ====\n",
        "agent_model = build_evo_agent(\n",
        "    visual_gene = test_gene[\"visual\"],\n",
        "    brain_gene  = test_gene[\"brain\"],\n",
        "    head_gene   = test_gene[\"head\"]\n",
        ")\n",
        "\n",
        "agent_model.summary()"
      ],
      "metadata": {
        "id": "mpCinp_NrkVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Lrt_hrMmrAzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evo_agent(model, gene, generation):\n",
        "  # ==== Test Frame Stack by Me ====\n",
        "  from datetime import datetime\n",
        "\n",
        "  # ==== Training Step ====\n",
        "  def train_step(states, target_qs, weights):\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_pred = model(states, training=True)\n",
        "        elementwise_loss = tf.keras.losses.Huber(reduction='none')(target_qs, q_pred)\n",
        "        loss = tf.reduce_mean(elementwise_loss * weights)  # â† IS weighting\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  # ==== Double Replay ====\n",
        "  def double_replay(model, batch_size, episode):\n",
        "\n",
        "    # === Safe Sample ===\n",
        "    sampled = memory.sample(batch_size)\n",
        "\n",
        "    if isinstance(memory, PrioritizedMultiStepReplayBuffer):\n",
        "        states, actions, rewards, next_states, dones, indices, weights = sampled\n",
        "    else:\n",
        "        states, actions, rewards, next_states, dones = sampled\n",
        "        indices = None\n",
        "        weights = np.ones_like(rewards)\n",
        "\n",
        "    # ==== Debug shape ====\n",
        "    if states.shape != (batch_size, 96, 96, 12):\n",
        "        print(f\"âš ï¸ [Episode {episode}] Sample shape mismatch: {states.shape}\")\n",
        "        return\n",
        "\n",
        "    # ==== Check ====\n",
        "    if states.shape != (batch_size, 96, 96, 12):\n",
        "      print(f\"âŒ Shape mismatch: got {states.shape}, skipping training\")\n",
        "      return\n",
        "\n",
        "    # ==== Double DQN target ====\n",
        "    next_q_main = model.predict(next_states, verbose=0)\n",
        "    # Predict next Q-values using main model (for action selection)\n",
        "    next_actions = np.argmax(next_q_main, axis=1)\n",
        "    # Predict next Q-values using target model (for evaluation)\n",
        "    next_q_target = target_model.predict(next_states, verbose=0)\n",
        "\n",
        "    # ==== Get current Q values ====\n",
        "    q_values = model.predict(states, verbose=0)\n",
        "\n",
        "    # ==== TD error ====\n",
        "    td_errors = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        target = rewards[i] if dones[i] else rewards[i] + gamma * next_q_target[i][next_actions[i]]\n",
        "        td_error = abs(target - q_values[i][actions[i]])  # â† TD error\n",
        "        td_errors.append(td_error)\n",
        "        q_values[i][actions[i]] = target\n",
        "\n",
        "    # Train on updated Q-values\n",
        "    # Call the tf.function-wrapped training step\n",
        "    # ==== Training Step ====\n",
        "    train_step(\n",
        "      tf.convert_to_tensor(states, dtype=tf.float32),\n",
        "      tf.convert_to_tensor(q_values, dtype=tf.float32),\n",
        "      tf.convert_to_tensor(weights[:, None], dtype=tf.float32),  # ðŸ‘ˆ shape=(batch_size, 1)\n",
        "    )\n",
        "\n",
        "    # ==== Update Priorities ====\n",
        "    if indices is not None:\n",
        "      memory.update_priorities(indices, td_errors)\n",
        "\n",
        "  # ==== Epsilon-greedy Parameters ====\n",
        "  # exploration rate\n",
        "  epsilon_holder = {\"epsilon\": 1.0}\n",
        "  # minimum exploration rate\n",
        "  epsilon_min = 0.01\n",
        "  # dacay rate for epsilon after each episode\n",
        "  epsilon_decay =100000 # 10000\n",
        "\n",
        "  epsilon_start = 1.0\n",
        "  epsilon_end = 0.05\n",
        "\n",
        "  #total_steps = 0\n",
        "\n",
        "  # ==== Action ====\n",
        "  # act(): chooses an action based on the epsilon-greedy policy.\n",
        "  # ==== Action Selection ====\n",
        "  def act(state, model, use_epsilon):\n",
        "\n",
        "      if use_epsilon:\n",
        "        if np.random.rand() < epsilon_holder[\"epsilon\"]:\n",
        "            return np.random.choice(action_size)  # Randomly select an action\n",
        "\n",
        "      q_values = model.predict(state[np.newaxis, ...], verbose=0)\n",
        "      return np.argmax(q_values[0])\n",
        "\n",
        "  # ==== Decay epsilon ====\n",
        "  def decay_epsilon(total_steps, episode):\n",
        "\n",
        "       epsilon_holder[\"epsilon\"] = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * total_steps / epsilon_decay)\n",
        "\n",
        "  # ==== Gene Name ====\n",
        "  gene_name = name_gene(gene)\n",
        "\n",
        "  # ==== Initialize Target Model ====\n",
        "  target_model = tf.keras.models.clone_model(model)\n",
        "  target_model.set_weights(model.get_weights())\n",
        "\n",
        "  # ==== Training Parameters ====\n",
        "  episodes = 3000 # More is good\n",
        "  batch_size = 128 # Minibatch size\n",
        "  gamma = 0.95 # Discount rate (~1 future is import)\n",
        "  max_step = 1000 # Max step per eposide\n",
        "  best_score = -np.inf\n",
        "\n",
        "  # ==== Before loop ====\n",
        "  steps = 0  # Initialize the steps\n",
        "\n",
        "  # ==== Record ====\n",
        "  rewards_per_episode = []\n",
        "\n",
        "\n",
        "  # ==== Initial memory type ====\n",
        "  memory = MultiStepReplayBuffer(capacity=100000, n_step=5, gamma=0.99)\n",
        "\n",
        "  # ==== Track whether switched ====\n",
        "  memory_switched = False\n",
        "\n",
        "  # ==== Clear memory to ensure consistent state shapes ====\n",
        "  # This prevents the ValueError if memory contains states from a non-stacked training run.\n",
        "  memory.clear()\n",
        "\n",
        "  # ===================\n",
        "  # ==== Training Loop ====\n",
        "  for episode in range(episodes):\n",
        "\n",
        "    # ==== Switching Exploration Strategy ====\n",
        "\n",
        "    use_epsilon = True\n",
        "\n",
        "    # ==== Initialize Environment ====\n",
        "    env = gym.make(\"CarRacing-v3\",\n",
        "                render_mode= None,\n",
        "                lap_complete_percent=0.95,\n",
        "                domain_randomize=True,\n",
        "                continuous=False)\n",
        "\n",
        "    # ==== Action History ====\n",
        "    action_history = []\n",
        "\n",
        "    # ==== Reset Environment ====\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    # ==== Switch to prioritized buffer =====\n",
        "    # Dynamic Switching Memory\n",
        "    if episode == 500 and not memory_switched:\n",
        "        new_memory = PrioritizedMultiStepReplayBuffer(capacity=100000, n_step=5, gamma=0.99)\n",
        "\n",
        "        # Copy old data over (optional)\n",
        "        for item in memory.buffer:\n",
        "            new_memory.tree.add(1.0, item)  # set default priority\n",
        "        memory = new_memory\n",
        "        memory_switched = True\n",
        "        print(\"ðŸ§  Switched to Prioritized Replay Buffer.\")\n",
        "\n",
        "    # ==== No-op  ====\n",
        "    for _ in range(50):\n",
        "        state, _, terminated, truncated, _ = env.step(0)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    # ==== Initialize stacking: RGB version ====\n",
        "    # Preprocess after no-op\n",
        "    # ==== Preprocess  ====\n",
        "    state = preprocess_frame(state, grayscale=False)\n",
        "    frame_buffer = deque([state] * 4, maxlen=4)\n",
        "    # ==== Stack State ====\n",
        "    stacked_state = np.concatenate(list(frame_buffer), axis=-1)\n",
        "\n",
        "    # ==== Episode Reward Initialize ====\n",
        "    episode_reward =0.0\n",
        "\n",
        "    # ==== Logic ====\n",
        "    # state â†’ select action â†’ step in env â†’ get next_state, reward â†’ modify â†’ store in memory\n",
        "\n",
        "    for time in range(max_step):\n",
        "      steps += 1  # Ensure epsilon dacay\n",
        "      # Choose action using epsilon-greedy policy\n",
        "      # act() â†’ strategy\n",
        "      # Use stacked state\n",
        "\n",
        "      # ==== Force Forward ====\n",
        "      # ==== NEW ====\n",
        "      if episode < 5 and time < 100:\n",
        "        # mode 1 (I recommend this one)\n",
        "        action = np.random.choice([1, 2, 3])\n",
        "\n",
        "        # mode 2\n",
        "        # action = np.random.choice([1, 2, 3, 4])\n",
        "      else:\n",
        "        action = act(stacked_state, model, use_epsilon)\n",
        "\n",
        "      action_history.append(action)\n",
        "\n",
        "      # ===== Take action ====\n",
        "      total_reward = 0\n",
        "\n",
        "      # ==== Frame Skipping ====\n",
        "      for _ in range(4):  # frame skip æ¬¡æ•¸\n",
        "          next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "          total_reward = total_reward + reward\n",
        "          if terminated or truncated:\n",
        "              break\n",
        "\n",
        "      done = terminated or truncated\n",
        "\n",
        "      # ==== Handle Next Step ====\n",
        "      if isinstance(next_state, tuple):\n",
        "        next_state = next_state[0]\n",
        "\n",
        "      next_state = preprocess_frame(next_state, grayscale=False)\n",
        "\n",
        "      # ==== Update Stack ====\n",
        "      frame_buffer.append(next_state)\n",
        "      stacked_next_state = np.concatenate(list(frame_buffer), axis=-1)\n",
        "\n",
        "      # ==== Reward Shaping ====\n",
        "      reward = reward_shaping(action, total_reward, stacked_next_state, None)\n",
        "\n",
        "      # ==== Remember ====\n",
        "      # Store experience in memory\n",
        "      # remember â†’ memorize (replay buffer)\n",
        "      # remember(stacked_state, action, reward, stacked_next_state, done)\n",
        "\n",
        "      # ==== Memorize ====\n",
        "      memory.store(stacked_state, action, reward, stacked_next_state, done)\n",
        "\n",
        "      # ==== Update state ====\n",
        "      stacked_state = stacked_next_state\n",
        "\n",
        "      # ==== Record reward ====\n",
        "      episode_reward = episode_reward + float(reward)\n",
        "\n",
        "      # If episode ends\n",
        "      if done:\n",
        "        rewards_per_episode.append(episode_reward)\n",
        "\n",
        "        # ====  Action Entropy ====\n",
        "        action_counts = np.array([action_history.count(a) for a in range(action_size)])\n",
        "\n",
        "        # Prevent log(0)\n",
        "        action_probs = action_counts / np.sum(action_counts) + 1e-8\n",
        "\n",
        "        action_entropy = scipy.stats.entropy(action_probs, base=2)\n",
        "\n",
        "        # ==== Plot =====\n",
        "\n",
        "        plot_training_progress(rewards_per_episode)\n",
        "\n",
        "        # ==== Print ====\n",
        "        eps = epsilon_holder[\"epsilon\"]\n",
        "        print(f\"Generation {generation} | Gene {gene_name} \\nðŸŽ¬ Episode {episode+1} | {datetime.now().strftime('%H:%M:%S')} | Reward: {episode_reward:.2f} | Epsilon: {eps:.4f} | Domain Randomization ={True} | Entropy: {action_entropy:.2f} bits\")\n",
        "        break\n",
        "\n",
        "    # ==== Decay Epsilon ====\n",
        "    decay_epsilon(steps, episode)\n",
        "\n",
        "    # ==== Train the model using replay memory ====\n",
        "\n",
        "    replay_start = 5000  # Tunable\n",
        "    if len(memory) > replay_start and len(memory) > batch_size:\n",
        "      double_replay(model, batch_size, episode=episode)\n",
        "\n",
        "    # ==== Reset Target Network ====\n",
        "    if episode % 10 == 0:\n",
        "      target_model.set_weights(model.get_weights())\n",
        "\n",
        "    # ==== Update Best Score ====\n",
        "    if episode_reward > best_score:\n",
        "      best_score = episode_reward\n",
        "      model.save(f'{gene_name}-best_model.keras')\n",
        "      print(f\"âœ… New Best Score {best_score}\")\n",
        "\n",
        "    print(\"Top actions in this episode:\", Counter(action_history).most_common())\n",
        "    print(\"======================================\")\n",
        "\n",
        "    # ==== Close Env ====\n",
        "    env.close()\n",
        "\n",
        "\n",
        "  print(f\"Generation {generation} | {gene_name} Agent Training complete after {episodes} episodes.\")\n",
        "  print(f\"Best model score of {gene_name}: {best_score:.2f}\")\n",
        "  return model, rewards_per_episode"
      ],
      "metadata": {
        "id": "4g83GzUMp1Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trained_model, training_rewards = train_evo_agent(test_model, test_gene, 0)"
      ],
      "metadata": {
        "id": "BKVIuFqntjQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "ohpED7J0rCZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(model, gene, generation):\n",
        "  # Re-initialize the environment for evaluation\n",
        "  # Use the same parameters as the training environment\n",
        "\n",
        "  # ==== Gene Name ====\n",
        "  gene_name = name_gene(gene)\n",
        "\n",
        "  # ==== GIF file ====\n",
        "  output_dir = f\"{gene_name}_test_real_gifs\"  # Name your file\n",
        "  os.makedirs(output_dir, exist_ok=True)  # Create it\n",
        "\n",
        "  # ==== Evaluation ====\n",
        "  evaluation_episodes = 100 # Number of evaluation\n",
        "  score = []\n",
        "\n",
        "  # ==== Initialize Environment ====\n",
        "  env = gym.make(\"CarRacing-v3\",\n",
        "                render_mode= 'rgb_array',\n",
        "                lap_complete_percent=0.95,\n",
        "                domain_randomize=True,\n",
        "                continuous=False)\n",
        "\n",
        "  # ==== Max Step = 1000 ====\n",
        "  max_step = 1000\n",
        "\n",
        "  for episode in range(evaluation_episodes):\n",
        "    # ==== Make GIF ====\n",
        "    gif_frames = []\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    frame = preprocess_frame(obs)\n",
        "\n",
        "    # Initialize frame buffer\n",
        "    frame_buffer = deque([frame]*NUM_FRAMES, maxlen=NUM_FRAMES)\n",
        "    stacked_state = np.concatenate(list(frame_buffer), axis=-1)  # shape: (96, 96, 12) if RGB\n",
        "\n",
        "    total_reward = 0\n",
        "\n",
        "\n",
        "    for time in range(max_step):\n",
        "      gif_frames.append(env.render())\n",
        "\n",
        "      # Step 1: Select action\n",
        "      q_values = model(stacked_state[np.newaxis, ...], training=False)[0]\n",
        "      action = np.argmax(q_values)\n",
        "\n",
        "      # Step 2: Interaction with env.\n",
        "      next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "      # Step 3: Preprocess frame\n",
        "      next_frame = preprocess_frame(next_obs)\n",
        "\n",
        "      # Step 4: Update frame buffer + stacked_state\n",
        "      frame_buffer.append(next_frame)\n",
        "      stacked_state = np.concatenate(list(frame_buffer), axis=-1)\n",
        "\n",
        "      # Step 5: Update reward\n",
        "      total_reward += float(reward)\n",
        "\n",
        "      # Step 6: Check if done\n",
        "      done = terminated or truncated\n",
        "      if done:\n",
        "          break\n",
        "\n",
        "    score.append(total_reward)\n",
        "    print(f\"Episode: {episode+1}/{evaluation_episodes}, Score: {total_reward:.2f}\")\n",
        "    gif_path = os.path.join(output_dir, f\"{gene_name}-episode_{episode+1:03d}-{total_reward:.0f}.gif\")\n",
        "    imageio.mimsave(gif_path, gif_frames, fps=30)\n",
        "\n",
        "  env.close()\n",
        "\n",
        "  # ==== Scores ====\n",
        "  average_score = np.mean(score)\n",
        "  max_score = np.max(score)\n",
        "  min_score = np.min(score)\n",
        "  std_score = np.std(score)\n",
        "\n",
        "  print(f'\\nGeneration {generation}|Gene {gene_name} Evaluation complete after {evaluation_episodes} Episodes\\n')\n",
        "  print(f\"Average Score: {average_score:.2f}\")\n",
        "  print(f\"Max Score: {max_score:.2f}\")\n",
        "  print(f\"Min Score: {min_score:.2f}\")\n",
        "  print(f\"Std Score: {std_score:.2f}\")\n",
        "\n",
        "  # ==== Visualization ====\n",
        "  def smooth(data, weight=0.9):\n",
        "      smoothed = []\n",
        "      last = data[0]\n",
        "      for point in data:\n",
        "          smoothed_val = last * weight + (1 - weight) * point\n",
        "          smoothed.append(smoothed_val)\n",
        "          last = smoothed_val\n",
        "      return np.array(smoothed)\n",
        "\n",
        "  smoothed = smooth(score)\n",
        "  std = np.std(score)\n",
        "\n",
        "\n",
        "  # ==== Use White Background ====\n",
        "  plt.rcParams['axes.facecolor'] = 'black'\n",
        "  plt.rcParams['figure.facecolor'] = 'black'\n",
        "  # ==== Plot ====\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  x = np.arange(len(score))\n",
        "\n",
        "  plt.plot(x, score, label='Raw Reward', alpha=0.3)\n",
        "\n",
        "  plt.plot(x, smoothed, label='Smoothed Reward', linewidth=2)\n",
        "\n",
        "  # ==== Std Band ====\n",
        "\n",
        "  plt.fill_between(x, smoothed - std, smoothed + std, alpha=0.2, color='skyblue', label='Â±1 Std')\n",
        "\n",
        "  # ==== Baseline ====\n",
        "  plt.axhline(y=500, color='red', linestyle='--', label='Pass Threshold')\n",
        "\n",
        "  # ==== Title and label ====\n",
        "  plt.title('Evaluation Reward Trend with Std Band')\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  # ==== Save Evaluated Model ====\n",
        "  model.save(f'{gene_name}.keras')\n",
        "\n",
        "  return average_score - 0.5 * std_score\n",
        "\n",
        "# ====Test Evaluation ====\n",
        "#evaluate_agent(trained_model, test_gene, 0)"
      ],
      "metadata": {
        "id": "DFHDRVs-rD_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evolution Loop"
      ],
      "metadata": {
        "id": "kkP_N26bA0Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross over"
      ],
      "metadata": {
        "id": "o7Q7AB0DC21h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_to_match(l1, l2, fill_value=None):\n",
        "    max_len = max(len(l1), len(l2))\n",
        "    l1 = l1 + [fill_value] * (max_len - len(l1))\n",
        "    l2 = l2 + [fill_value] * (max_len - len(l2))\n",
        "    return l1, l2"
      ],
      "metadata": {
        "id": "Y0CMPiE5NB9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crossover_gene(parent1, parent2):\n",
        "    visual1, brain1, head1 = parent1[\"visual\"], parent1[\"brain\"], parent1[\"head\"]\n",
        "    visual2, brain2, head2 = parent2[\"visual\"], parent2[\"brain\"], parent2[\"head\"]\n",
        "\n",
        "    # === Visual Gene Crossover ===\n",
        "    visual = [random.choice([v1, v2]) for v1, v2 in zip(visual1, visual2)]\n",
        "\n",
        "    # === Brain Gene Crossover ===\n",
        "    brain1, brain2 = pad_to_match(brain1, brain2)\n",
        "    brain = []\n",
        "    for b1, b2 in zip(brain1, brain2):\n",
        "        if b1 is None or b2 is None:\n",
        "            brain.append(None if b1 is None and b2 is None else b1 if b2 is None else b2)\n",
        "        else:\n",
        "            brain.append(random.choice([b1, b2]))\n",
        "\n",
        "    # === Head Gene Crossover ===\n",
        "    head1, head2 = pad_to_match(head1, head2)\n",
        "    head = [random.choice([h1, h2]) for h1, h2 in zip(head1, head2)]\n",
        "\n",
        "    return {\"visual\": visual, \"brain\": brain, \"head\": head}"
      ],
      "metadata": {
        "id": "az4y9iFxCJJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mutate Gene"
      ],
      "metadata": {
        "id": "zWP7vy2TC5Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mutate_gene(gene, p=0.1):\n",
        "    visual = gene[\"visual\"][:]\n",
        "    brain = gene[\"brain\"][:]\n",
        "    head = gene[\"head\"][:]\n",
        "\n",
        "    # === Mutate visual gene ===\n",
        "    if random.random() < p:\n",
        "        visual[0] = random.randint(0, 3)  # contrast_times\n",
        "    if random.random() < p:\n",
        "        visual[1] = random.randint(0, 3)  # sobel_times\n",
        "    if random.random() < p:\n",
        "        visual[2] = round(random.uniform(1.0, 3.0), 1)  # alpha\n",
        "    if random.random() < p:\n",
        "        visual[3] = random.choice([True, False])  # residual\n",
        "\n",
        "    # === Mutate brain gene ===\n",
        "    for i in range(len(brain)):\n",
        "        if random.random() < p:\n",
        "            if brain[i] is not None:\n",
        "                brain[i] = random.choice([None, 64, 128, 256, 512])\n",
        "            else:\n",
        "                brain[i] = random.choice([64, 128, 256])\n",
        "\n",
        "    # === Mutate head gene ===\n",
        "    for i in range(len(head)):\n",
        "        if random.random() < p:\n",
        "            head[i] = random.choice(['type0', 'type1', 'type2', 'type3', 'type4', 'type5', None])\n",
        "\n",
        "    return {\n",
        "        \"visual\": visual,\n",
        "        \"brain\": brain,\n",
        "        \"head\": head\n",
        "    }"
      ],
      "metadata": {
        "id": "IM3w3xvLDBGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== TEST ====\n",
        "p1 = random_gene()\n",
        "p2 = random_gene()\n",
        "\n",
        "print(f'p1: {p1}')\n",
        "print(f'p2: {p2}')\n",
        "\n",
        "child = crossover_gene(p1, p2)\n",
        "print(f'child: {child}')\n",
        "mutated_gene = mutate_gene(child)\n",
        "print(f'mutated_gene: {mutated_gene}')\n"
      ],
      "metadata": {
        "id": "IyvAX4vJESoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evolution"
      ],
      "metadata": {
        "id": "a_NgNQZ4OjHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras import backend as K\n",
        "import gc\n",
        "\n",
        "# ==== Set Evolution Parameters ====\n",
        "n_generations = 3\n",
        "population_size = 5\n",
        "top_k = 3\n",
        "\n",
        "# ==== Original Gene (Ancestor) ====\n",
        "ancestor_gene = {\n",
        "    \"visual\": [2, 0, 2.0, False],\n",
        "    \"brain\": [512, 256, 128, 64],\n",
        "    \"head\": ['type1', 'type2', 'type3']\n",
        "}\n",
        "\n",
        "# ==== Initialize Population with Ancestor ====\n",
        "population = [ancestor_gene] + [random_gene() for _ in range(population_size - 1)]\n",
        "\n",
        "# ==== Log container ====\n",
        "log = []\n",
        "\n",
        "# ==== Evolution Loop ====\n",
        "for generation in range(n_generations):\n",
        "    print(f\"\\n==== Generation {generation+1} ====\")\n",
        "\n",
        "    fitness_scores = []\n",
        "\n",
        "    for gene in population:\n",
        "        model = build_evo_agent(\n",
        "            visual_gene=gene[\"visual\"],\n",
        "            brain_gene=gene[\"brain\"],\n",
        "            head_gene=gene[\"head\"]\n",
        "        )\n",
        "        print(f\"Model built for: {name_gene(gene)}\")\n",
        "        train_evo_agent(model, gene, generation)\n",
        "        score = evaluate_agent(model, gene, generation)\n",
        "        fitness_scores.append(score)\n",
        "\n",
        "        # === Append to log ===\n",
        "        log.append({\n",
        "            \"generation\": generation + 1,\n",
        "            \"gene_name\": name_gene(gene),\n",
        "            \"visual\": gene[\"visual\"],\n",
        "            \"brain\": gene[\"brain\"],\n",
        "            \"head\": gene[\"head\"],\n",
        "            \"fitness\": score,\n",
        "            \"average_score\": np.mean(fitness_scores),\n",
        "        })\n",
        "\n",
        "        print(f\"Generation: {generation}|Gene Name: {name_gene(gene)} | Fitness: {score:.2f}\")\n",
        "\n",
        "        del model\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "    # ==== Select Top Genes ====\n",
        "    selected = [gene for _, gene in sorted(zip(fitness_scores, population), reverse=True)[:top_k]]\n",
        "\n",
        "    # ==== Create Next Generation ====\n",
        "    next_generation = []\n",
        "    while len(next_generation) < population_size:\n",
        "        parent1, parent2 = random.sample(selected, 2)\n",
        "        child = crossover_gene(parent1, parent2)\n",
        "        child = mutate_gene(child)\n",
        "        next_generation.append(child)\n",
        "\n",
        "    population = next_generation\n",
        "\n",
        "# ==== Save Log as CSV ====\n",
        "df = pd.DataFrame(log)\n",
        "df.to_csv(\"evolution_log.csv\", index=False)\n",
        "print(\"âœ… Log saved as evolution_log.csv\")"
      ],
      "metadata": {
        "id": "5xSIgEJ-_Mhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ohS4hlfdnWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMiz5r/ML+RjOtFTrqJw7cR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}